[{"id":"c8a7e9308e269c0b57685bf049081ac1","title":"决策树","content":"\r\n","slug":"决策树","date":"2023-02-28T08:34:59.000Z","categories_index":"","tags_index":"","author_index":"碔砆"},{"id":"806a34f762f55e51b9922352f8456268","title":"DenseNet","content":"DenseNet\r\nIntroduction\r\n从ResNet到DenseNet\r\n在ResNet中，我们通过的方式来训练模型\r\n而从这个式子中，我们可以很容易的联想的Taylor公式\r\n而DenseNet就是在ResNet的基础上，对该公式进行了进一步的展开\r\nResNet将f(x)分为了一个简单的线性项x和一个复杂的非线性项，而DenseNet就在这基础上将f(x)分为更多的部分\r\n\r\n\r\nResNet块和Dense块的比较，Dense块使用连结\r\n\r\nResNet和DenseNet的关键区别在于，DenseNet输出是连接（用图中的[,]表示）而不是如ResNet的简单相加\r\n因此，在应用越来越复杂的函数序列后，我们执行从x到其展开式的映射\r\n\r\n最后，将这些展开式结合到多层感知机中，再次减少特征的数量。\r\n实现起来非常简单：我们不需要添加术语，而是将它们连接起来\r\n\r\n\r\n稠密连接示意图\r\n\r\n代码\r\npytorch官方代码\r\nDense块\r\nclass DenseBlock(nn.Module):\n    def __init__(self, num_convs, input_channels, num_channels):\n        super(DenseBlock, self).__init__()\n        layer = []\n        # 稠密部分\n        for i in range(num_convs):\n            layer.append(conv_block(\n                num_channels * i + input_channels, num_channels))\n        self.net = nn.Sequential(*layer)\n\n    def forward(self, X):\n        for blk in self.net:\n            Y = blk(X)\n            # 连接通道维度上每个块的输入和输出\n            X = torch.cat((X, Y), dim=1)\n        return X\r\n与之前完成的ResNet对比可看出，ResNet块中，用于拟合的shortcut，在这之中增加了若干个卷积层来达到稠密的效果\r\n","slug":"DenseNet","date":"2023-02-27T11:23:20.000Z","categories_index":"AI学习,经典模型","tags_index":"AI学习,经典模型","author_index":"碔砆"},{"id":"8f025f73c51b2b366e2e8ff8dcf261a1","title":"ResNet","content":"ResNet\r\n论文原址\r\n李沐读论文——ResNet\r\nResNet博客详解\r\n代码\r\npytorch官方代码\r\n嵌套函数\r\n假设有一类特定的神经网络架构F，它包括学习速率和其他超参数设置。\r\n对于所有，存在一些参数集（例如权重和偏置），这些参数可以通过在合适的数据集上进行训练而获得\r\n现在假设是我们真正想要找到的函数，如果是，那我们可以轻而易举的训练得到它，但通常我们不会那么幸运\r\n相反，我们将尝试找到一个函数，这是我们在F中的最佳选择\r\n例如，给定一个具有X特性和y标签的数据集，我们可以尝试通过解决以下优化问题来找到它\r\n\r\n而又该如何去寻找呢，ResNet给了我们一个思路：建造一个更强大的架构F'\r\n换句话说，我们预测比更加近似，但这又必须建立在一个前提之上：\r\n因此，我们引入一个嵌套函数来完成这一前提\r\n\r\n通过每一层函数都嵌套于前一层函数之上，我们可以保证函数更接近真函数\r\n也只有在较复杂的函数类包含较小的函数类时，我们才可以保证最优化的实现\r\n对于深度神经网络，如果我们能将新添加的层训练成恒等映射（identity\r\nfunction），新模型和原模型将同样有效\r\n同时，由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差\r\n残差块\r\n假设我们原始输入是x，我们的理想映射是f(x)\r\n左图虚线框中的部分需要直接拟合出该映射f(x)，而右图虚线框中的部分则需要拟合出残差映射f(x)-x\r\n一个残差块有2条路径f(x)和x，f(x)路径拟合残差，不妨称之为残差路径，x路径为恒等映射，称之为shortcut\r\n以本节开头提到的恒等映射作为我们希望学出的理想映射f(x)，我们只需将右图虚线框内上方的加权运算（如仿射）的权重和偏置参数设成0，那么f(x)即为恒等映射\r\n与其让f直接学习潜在的映射，不如去学习残差(x)−x，即f(x):=−x，这样原本的前向路径上就变成了f(x)+x，用f(x)+x来拟合\r\n当理想映射f(x)极接近于恒等映射时，残差映射也易于捕捉恒等映射的细微波动\r\n\r\n\r\n一个正常块和一个残差块\r\n\r\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass Residual(nn.Module):\n    def __init__(self, input_channels, num_channels,\n                 use_1x1conv=False, strides=1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(input_channels, num_channels,\n                               kernel_size=3, padding=1, stride=strides)\n        self.conv2 = nn.Conv2d(num_channels, num_channels,\n                               kernel_size=3, padding=1)\n        # 是否使用1*1卷积层来改变通道数\n        if use_1x1conv:\n            self.conv3 = nn.Conv2d(input_channels, num_channels,\n                                   kernel_size=1, stride=strides)\n        else:\n            self.conv3 = None\n        self.bn1 = nn.BatchNorm2d(num_channels)\n        self.bn2 = nn.BatchNorm2d(num_channels)\n\n    def forward(self, X):\n        Y = F.relu(self.bn1(self.conv1(X)))\n        Y = self.bn2(self.conv2(Y))\n        if self.conv3:\n\t        # shortcut旁路\n            X = self.conv3(X)\n        Y += X\n        return F.relu(Y)\r\n网络\r\nResNet内部是若干个残差块的串联\r\nResNet的设计有如下特点： - 与plain\r\nnet相比，ResNet多了很多“旁路”，即shortcut路径，其首尾圈出的layers构成一个Residual\r\nBlock； - ResNet中，所有的Residual\r\nBlock都没有pooling层，降采样是通过conv的stride实现的； -\r\n分别在conv3_1、conv4_1和conv5_1 Residual Block，降采样1倍，同时feature\r\nmap数量增加1倍 - 通过Average\r\nPooling得到最终的特征，而不是通过全连接层； -\r\n每个卷积层之后都紧接着BatchNorm layer\r\n通过调整block内的channel数量以及堆叠的block数量，就可以很容易地调整网络的宽度和深度，来得到不同表达能力的网络，而无需去另外担心退化问题，只需要有足够多的样本就能得到良好的效果\r\n\r\n\r\nResNet优化效果的error\r\nface对比\r\n\r\n可以看出，增加shortcut之后，ResNet的loss优化有了显著提升，并且可以感受到，与其他模型相比，在浅层处是否增加short的差异并不明显，而在深层处格外明显\r\n","slug":"ResNet","date":"2023-02-27T11:23:08.000Z","categories_index":"AI学习,经典模型,论文精读","tags_index":"AI学习,论文精读,经典模型","author_index":"碔砆"},{"id":"232f9f51309688e33bd70bd8c0ccce09","title":"批量规范化","content":"\r\n","slug":"批量规范化","date":"2023-02-27T11:23:00.000Z","categories_index":"","tags_index":"","author_index":"碔砆"},{"id":"9d8d702581fafe91f0e84d17245c7b9e","title":"最大公因数GCD","content":"最大公因数GCD\r\n原理\r\n利用了欧几里得算法，即辗转相除法\r\n核心等式——gcd(a,b) = gcd(b,a mod b)\r\n证明\r\na可以表示成a = kb + r（a，b，k，r皆为正整数)\r\n假设d是a,b的一个公约数，记作d|a,d|b) 即a和b都可以被d整除。\r\n而r = a - kb，两边同时除以d\r\nr/d=a/d-kb/d，由等式右边可知m=r/d为整数，因此d|r\r\n因此d也是b,a mod b的公约数。\r\n因(a,b)和(b,a mod b)的公约数相等，则其最大公约数也相等，得证。\r\n算法实现\r\n\r\n简单方法 int gcd(int m,int n)\n&#123;    \n    int t,r;    \n    if (m&lt;n)        &#x2F;&#x2F;为了确保是大数除小数    \n    &#123;        \n        t&#x3D;m;        \n        m&#x3D;n;       \n        n&#x3D;t;    \n    &#125;    \n \n    while((m%n)!&#x3D;0) &#x2F;&#x2F;辗转相除    \n    &#123;        \n        r&#x3D;m%n;        \n        m&#x3D;n;        \n        n&#x3D;r;    \n    &#125;   \n \n    return n;\n&#125;\r\n递归方法 int gcd(int x, int y)\n&#123;\tif (y)\t\t\t\n            return gcd(y, x%y);\t\t\n        else\t\t\t\n            return x;\n&#125;\r\n位运算算法 int gcd(int x, int y)\n&#123;\n    while(y^&#x3D;x^&#x3D;y^&#x3D;x%&#x3D;y);\n    return x;\n&#125;\r\n\r\n","slug":"最大公因数GCD","date":"2023-02-25T14:29:59.174Z","categories_index":"算法","tags_index":"算法","author_index":"碔砆"},{"id":"f466839ba23e30f0626022c248cd73f5","title":"GoogLeNet","content":"GoogLeNet\r\nIntroduction\r\nGoogLeNet吸收了NiN中串联网络的思想，并在此基础上做了改进\r\n这篇论文的一个重点是解决了什么样大小的卷积核最合适的问题\r\n本文的一个观点是，有时使用不同大小的卷积核组合是有利的\r\n模型结构——Inception块\r\n在GoogLeNet中，基本的卷积块被称为Inception块（Inception block）\r\n\r\n如上图所示，Inception块由四条并行块组成\r\n前3条路径使用窗口大小为,和的卷积层，从不同空间大小中提取信息\r\n中间两条通道先进行的卷积层，从而减少模型复杂度\r\n第四条路径使用最大汇聚层，然后使用卷积层来改变通道数\r\n这四条路径都使用合适的填充来使输入与输出的高和宽一致，最后我们将每条线路的输出在通道维度上连结，并构成Inception块的输出\r\n在Inception块中，通常调整的超参数是每层输出通道数\r\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nclass Inception(nn.module):\n\tdef __init__(self, in_channels, c1, c2, c3, c4, **kwargs):\n\t# c1,c2,c3,c4 是每条路径的输出通道数\n\t\tsuper(Inception, self).__init__(**kwargs)\n\t\tself.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n\t\t# 线路1 单1*1卷积层\n\t\tself.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n\t\tself.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n\t\t# 线路2 1*1卷积层接3*3卷积层\n\t\tself.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n\t\tself.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=1)\n\t\t# 线路3 1*1卷积层接5*5卷积层\n\t\tself.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n\t\tself.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n\t\t# 线路4 最大池化层后接1*1卷积层\n\n\tdef forward(self, x):\n\t\tp1 = F.relu(self.p1_1(x))\n\t\tp2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n        p4 = F.relu(self.p4_2(self.p4_1(x)))\n        return torch.cat((p1, p2, p3, p4), dim=1)\n        # 在通道上联结输出\r\n关于Inceptional块为何有效，可以参考滤波器的相关理论：\r\n用各种滤波器尺寸探索图像，这意味着不同大小的滤波器可以有效地识别不同范围的图像细节\r\n同时，我们可以为不同的滤波器分配不同数量的参数\r\n模型结构——GoogLeNet\r\nGoogLeNet一共使用9个Inception块和全局平均汇聚层的堆叠来生成其估计值\r\nInception块之间的最大汇聚层可降低维度\r\n第一个模块类似于AlexNet和LeNet，Inception块的组合从VGG继承，全局平均汇聚层避免了在最后使用全连接层\r\n\r\n\r\nGoogLeNet结构\r\n\r\n逐层实现\r\nb1 = nn.Sequential(\n\tnn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3), nn.ReLU(),\n\tnn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n)\n# 7 * 7 卷积层\n\nb2 = nn.Sequential(\n\tnn.Conv2d(64, 64, kernel_size=1), nn.ReLU(),\n\tnn.Conv2d(64, 192, kernel_size=3, padding=1), nn.ReLU(),\n\tnn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n)\n# 3 * 3 卷积层\r\n在第三个模块中，串联了两个Inception块，第一个Inception块的输出通道数为，四个路径之间的输出通道数之比是\r\n第二个和第三个路径首先把输入通道数减到96和16，再连接第二个Inception块\r\n第二个Inception块的输出通道数为，四个路径之间的输出通道数之比是\r\n第二个和第三个路径首先把输入通道数减到128和32\r\nb3 = nn.Sequential(\n\tInception(192, 64, (96, 128), (16, 32), 32),\n\tInception(256, 128, (128, 192), (32, 96), 64),\n\tnn.MaxPool2d(kernel_2d=3, stride=2, padding=1)\n)\r\n第四个模块中串联了5个Inception块，其中路径输入输出通道数的思想与第三个模块中类似\r\n都是含卷积层的第二条路径输出通道数最多，其次是仅含卷积层的第一条通道，之后是含卷积层的第三条通道和含最大池化层的第四条通道，其中第二条和第三条通道都会按比例减小通道数\r\nb4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64),\n                   Inception(512, 160, (112, 224), (24, 64), 64),\n                   Inception(512, 128, (128, 256), (24, 64), 64),\n                   Inception(512, 112, (144, 288), (32, 64), 64),\n                   Inception(528, 256, (160, 320), (32, 128), 128),\n                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\r\n第五个模块中串联了2个Inception块，其中参数的设置思想与之前类似，值得关注的是，第五个模块后链接着输出层，该输出层与NiN一样采取全局平均池化层\r\nb5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128),\n                   Inception(832, 384, (192, 384), (48, 128), 128),\n                   nn.AdaptiveAvgPool2d((1,1)),\n                   nn.Flatten())\r\n","slug":"GoogLeNet","date":"2023-02-24T12:58:30.000Z","categories_index":"AI学习,经典模型","tags_index":"AI学习,经典模型","author_index":"碔砆"},{"id":"c8663499333ced86e35b7c271cd82e06","title":"NiN","content":"NiN——网络中的网络\r\nInroduction\r\nLeNet、AlexNet和VGG都有一个共同的设计模式：\r\n通过一系列的卷积层与汇聚层来提取空间结构特征，然后通过全连接层对特征的表征进行处理\r\nAlexNet和VGG对LeNet的提升主要在于如何扩大和加深这两个模块\r\n而这之中就发现了一个问题：卷积层后的第一个全连接层的参数过于庞大\r\n那么，我们是否可以在DNN的前期训练中就使用全连接层呢？\r\nNiN——网络中的网络给出了解决方法\r\n模型结构——NiN块\r\n回想一下，卷积层的输入和输出由四维张量组成，张量的每个轴分别对应样本、通道、高度和宽度\r\n另外，全连接层的输入和输出通常是分别对应于样本和特征的二维张量\r\n从中，NiN给出了一种构想：在每个像素处，对高度和宽度分别应用一个全连接层\r\n从另一个角度看，即将空间维度中的每个像素视为单个样本，将通道维度视为不同特征\r\nNiN块以一个普通卷积层开始，后面是两个的卷积层\r\n这两个卷积层充当带有ReLU激活函数的逐像素全连接层\r\n关于卷积层的性质详见\r\n卷积神经网络一文``\r\n\r\nimport torch\nfrom torch import nn\n\ndef nin_block(in_channels, out_channels, kernel_size, stride, padding)\n\treturn nn.Sequential(\n\t\tnn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n\t\tnn.ReLU(),\n\t\tnn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(),\n\t\tnn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU()\n\t)\r\n模型结构——NiN网络\r\nNiN和AlexNet之间的一个显著区别是NiN完全取消了全连接层\r\n相反，NiN使用一个NiN块，其输出通道数等于标签类别的数量，最后放一个全局平均池化层，生成一个对数几率\r\n这么做可以显著减少模型所需的参数数量，但在实践中发现这样有时会增加训练时间\r\nNiN = nn.Sequential(\n\tnin_block(1, 96, kernel_size=11, stride=4, padding=0),\n\tnn.MaxPool2d(3, stride=2),\n\tnin_block(96, 256, kernel_size=5, stride=4, padding=2),\n\tnn.MaxPool2d(3, stride=2),\n\tnin_block(256, 384, kernel_size=3, stride=1, padding=1),\n\tnn.MaxPool2d(3, stride=2),\n\tnn.Dropout(p=0.5),\n\tnin_block(384, 10, kernel_size=3, stride=1, padding=1),\n\t# 这里的10由数据集决定\n\tnn.AdaptiveAvgPool2d((1,1)),\n\tnn.Flatten()\n\t# 将四维的输出转换为二维的输出，其形状大小为(批量大小， 10)\n)\r\nNiN的提出主要是为了减少全连接所带来的参数过大，从而引发的对模型显存的挑战，但相应的，NiN对模型训练的速度也存在着一定的影响，在当下模型显存不断发展的情况下，NiN技术逐渐被淘汰\r\n","slug":"NiN","date":"2023-02-24T11:41:11.000Z","categories_index":"AI学习,经典模型","tags_index":"AI学习,经典模型","author_index":"碔砆"},{"id":"e488f86556a876ba1fc98ee3101d0802","title":"VGG","content":"VGG\r\nVGG论文\r\nd2l\r\nIntrodution\r\nAlexNet的成果，证明了CNN在图像分类领域的强大性能，但它没有提供一个通用的模板来指导后续的研究人员设计新的网络\r\n与芯片设计中工程师从放置晶体管到逻辑元件再到逻辑块的过程类似，神经网络架构的设计也逐渐变得更加抽象\r\n研究人员开始从单个神经元的角度思考问题，发展到整个层，现在又转向块，重复层的模式\r\n而使用块的想法首先出现在牛津大学的视觉几何组（visual geometry\r\ngroup）的VGG网络中\r\n通过使用循环和子程序，可以很容易地在任何现代深度学习框架的代码中实现这些重复的架构\r\n模型结构——VGG块\r\n从之前AlexNet论文中所提出的CNN模型中，我们可以总结出CNN模型的的基本组成为：\r\n- 带填充和步幅来调整和保证分辨率的卷积层 - 非线性激活函数 - 池化层\r\n而VGG块与上面所总结的类似，由一系列卷积层，相应的激活函数和一个池化层所组成\r\n在VGG原论文中，作者使用了带有卷积核、填充为1（保持高度和宽度）的卷积层，和带有汇聚窗口、步幅为2（每个块后的分辨率减半）的最大汇聚层\r\nimport torch\nfrom torch import nn\n\ndef vgg_block(nums_conv, in_channels, out_channels):\n# nums_conv 卷积层数量\n# in_channels 输入通道数\n# out_channels 输出通道数\n\tlayers = []\n\tfor _ in range(nums_conv):\n\t\tlayers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n\t\tlayers.append(nn.ReLU())\n\t\tin_channels = out_channels\n\tlayers.append(nn.MaxPool2d(kernel_size=1))\n\treturn nn.Sequential(*layers)\r\n模型结构——VGG网络\r\n\r\n与AlexNet和LeNet一样，VGG仍然是以卷积块和全连接块两部分组成，其主要区别在于，VGG拥有着较为灵活的VGG块\r\n通过调整每个VGG块的超参数，就可以定制不同的VGG模型，下面以VGG-11为例\r\nconv_arch=((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))\n# vgg-11的vgg块超参数\r\ndef vgg(conv_arch):\n\tconv_blks = []\n\tin_channels = 1\n\tfor (nums_conv, out_channels) in conv_arch:\n\t\tconv_blks.append(vgg_block(nums_conv, in_channels, out_channels))\n\t\tin_channels = out_channels\n\t# 卷积部分\n\treturn nn.Sequential(\n\t\t*conv_blks, nn.Flatten(),\n\t\tnn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(p=0.5),\n\t\tnn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(p=0.5),\n\t\tnn.Linear(4096, 10)\n\t\t# 这里的10由数据集决定\n\t)\r\n","slug":"VGG","date":"2023-02-24T08:01:55.000Z","categories_index":"AI学习,经典模型","tags_index":"AI学习,经典模型","author_index":"碔砆"},{"id":"2c4c0bf1dd06ffaa7e2ce0054928cf5c","title":"如何精读文章","content":"如何精读文章\r\n这篇文章主要参考了李沐老师的讲解，以及我本科导师的推荐方法，外加上本人平日阅读时的感受\r\n仅代表个人见解，欢迎交流讨论\r\n阅读方法——三步读法\r\n李沐老师介绍的三步读法大致按照以下几个步骤\r\n导读\r\n在导读，也就是第一次阅读时，我们主要关注文章的标题、摘要和结论\r\n标题，也就是我们正常情况下去阅读这篇文章的动力\r\n从标题中，我们可以快速获取这篇文章所对应的领域以及所提及的问题\r\n而一篇文章的摘要更是对标题的补充，会较为完善的提及这篇文章所关注的问题，所使用的模型和方法，以及最后的产出与贡献\r\n读完这些部分，此时已经对一篇文章的大致思路有了一定的了解，所以转而去阅读结论\r\n在结论中，作者首先会基于创新点和实验结果对整篇文章进行一个总结，之后会对于这结论的应用以及未来发展进行一定的展望与思考\r\n在完成导读后，目前我们从这篇论文中提取的信息有： - 文章所在的领域 -\r\n文章所完成的工作，即创新点 - 对文章所完成的工作起到的效果的结论\r\n到此，我们已完成初步阅读一篇文章，之后再结合所提取的信息，与我们的科研目的是否匹配等因素去决定是否要进入第二次阅读\r\n通读\r\n","slug":"如何精读文章","date":"2023-02-21T06:16:10.000Z","categories_index":"论文精读","tags_index":"论文精读","author_index":"碔砆"},{"id":"c6ff40f5693e190d69588e9c4620b737","title":"AlexNet","content":"AlexNet\r\n论文原址\r\n跟李沐读论文——AlexNet\r\n三步读法\r\n1. 导览——标题、摘要、结论\r\n标题\r\nImageNet Classification with Deep Convolutional Neural Networks\r\n从中可以感受到几个重点：\r\n数据集：ImageNet\r\n当时最大的数据集，也是DNN与传统机器学习相比，发挥不佳的数据集，是当时DNN在CV领域研究的重难点\r\n创新点：Deep Convolutional Nerual Networks\r\n虽然在如今的视角下，CNN已经在DNN领域得到了非常广泛的应用，但在当时的研究背景下，CNN是一个非常新鲜且陌生的概念，因此可以被认为是本文的创新点\r\n摘要\r\n在摘要中，这篇文章主要分为以下几个部分：\r\n\r\n总起：以LSVRC竞赛为例，展示了模型在ImageNet上的效果非常好\r\n介绍：大致介绍了模型的结构和参数数量，以及当时少见的神经元数量\r\n训练：表示本文采取GPU训练（在当时已经逐渐流行的一种训练方式）\r\n技巧：介绍了本文为减少过拟合所采取的技巧——dropout\r\n\r\n从上可看出，本文的摘要突出了一件事——我们网络的效果好\r\n（虽然本文是非常经典的论文，但这篇文章所采取的摘要结构不建议作为参考）\r\n结论\r\n非常罕见，本篇文章中并没有Consequence部分，故本部分以Discussion作为代替\r\n总结：对本篇文章中CNN所取得的成果进行总结与肯定，并且对未来进行展望\r\n消融：总结了实验中，减少CNN模型深度所带来的影响，得出结论：CNN模型与模型深度具有非常相关的联系\r\n展望：回顾实验，表示了本文在有监督学习下的效果很好，而无需另外的无监督学习的预处理\r\n这段话也使得，DNN领域一时间将研究的重点转向了有监督学习之中\r\n展望：如果未来的计算资源越来越强大，则可以得到更好的效果\r\n不足：承认当下的技术与人类能力还存在较大差距，对未来的工作进行期待\r\n直观感受与简单总结\r\n这篇文章，作为CNN领域的开山之作，是一篇非常具有开创性的文章\r\n重新回到2012年的视角去看待，这篇文章最吸引人的地方在于：\r\n在CV领域提出了了一个全新的且超越过去所有技术的新概念——CNN\r\n而在摘要与结论中也能感受到AlexNet所关心的重点——模型结构、效果和采取的技巧Dropout\r\n2. 通读\r\nReadPaper——我的论文笔记\r\n3. 精读\r\n数据集\r\n在数据集方面，AlexNet摒弃了以往CV领域所使用的特征流水线技术，开始转而使用带标签的数据集\r\n从而带火了端到端(End to\r\nEnd，指训练与使用的输入与输出相对应)在CV领域的广泛应用\r\n此处特另外展现一下过去的特征流水线： -\r\n获取一个有趣的数据集。在早期，收集这些数据集需要昂贵的传感器（在当时最先进的图像也就100万像素）\r\n- 根据光学、几何学、其他知识以及偶然的发现，手工对特征数据集进行预处理 -\r\n通过标准的特征提取算法，如SIFT（尺度不变特征变换） (Lowe,\r\n2004)和SURF（加速鲁棒特征） (Bay et al.,\r\n2006)或其他手动调整的流水线来输入数据 -\r\n将提取的特征送入最喜欢的分类器中（例如线性模型或其它核方法），以训练分类器\r\n模型详解\r\n\r\n每一个框表示每一层输入输出数据的大小，小方块表示卷积核\r\n从上图可以看出，输入的高宽逐渐变小，深度，即通道数逐渐增加\r\n这表示着模型中逐步压缩图片数据的一个过程，而通道数可以理解为每个通道数表示一种模式的提取\r\n经过这个过程，一个样本就经由逐层的加工最终转化为一个特征向量，而在决策空间中，我们用这个特征向量代表对于的样本，因此我们可以对这个向量来进行其他的处理，这也是DNN的精髓所在\r\n减少过拟合\r\n数据增强\r\nPCA正则化\r\nDropout算法\r\n代码\r\n\r\n\r\nAlexNet和LeNet的比较\r\n\r\nLeNet和AlexNet模型结构的比较\r\nimport torch\nfrom torch import nn\n\nAlexNet = nn.Sequential(\n\t# 输入经过预处理\n\tnn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),\n\tnn.MaxPool2d(kernel_size=3, stride=2),\n\t# 使用一个11*11的大窗口，外加96个输出通道来快速提取图像特征\n\tnn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),\n\tnn.MaxPool2d(kernel_size=3, stride=2),\n\t# 切换成小窗口，通过填充来保证输入输出大小一致\n\tnn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),\n\tnn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),\n\tnn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),\n\tnn.MaxPool2d(kernel_size=3, stride=2),\n\t# 使用了3个卷积层和较小的窗口\n\tnn.flatten(),\n\tnn.Linear(4096, 4096), nn.ReLU(),\n\tnn.Dropout(p=0.5),\n\tnn.Linear(4096, 4096), nn.ReLU(),\n\tnn.Dropout(p=0.5),\n\tnn.Linear(4096, 10)\n\t# 最后的这个10由训练集决定\n)\r\n","slug":"AlexNet","date":"2023-02-20T12:55:00.000Z","categories_index":"AI学习,论文精读,经典模型","tags_index":"AI学习,论文精读,经典模型","author_index":"碔砆"},{"id":"139386a48ee9ff8eae54fecf132447a0","title":"LeNet","content":"LeNet\r\nIntroduction\r\n这是世界上最早发布的卷积神经网络之一，由AT&amp;T贝尔实验室的研究员Yann\r\nLeCun在1989年提出的（并以其命名）\r\n目的是识别图像 MNIST 中的手写数字\r\n当时，Yann\r\nLeCun发表了第一篇通过反向传播成功训练卷积神经网络的研究，这项工作代表了十多年来神经网络研究开发的成果\r\n模型结构\r\n整体上看，LeNet主要分为两个部分： - 卷积编码器：由两个卷积层组成 -\r\n全连接层密集块：由三个全连接层组成\r\n\r\n每个卷积块中的基本单元是一个卷积层、一个sigmoid激活函数和平均汇聚层\r\n每个卷积层使用卷积核和一个sigmoid激活函数\r\n这些层将输入映射到多个二维特征输出，通常同时增加通道的数量\r\n为了将卷积块的输出传递给稠密块，我们必须在小批量中展平每个样本\r\n换言之，我们将这个四维输入转换成全连接层所期望的二维输入\r\n这里的二维表示的第一个维度索引小批量中的样本，第二个维度给出每个样本的平面向量表示\r\nLeNet的稠密块有三个全连接层，分别有120、84和10个输出\r\n代码\r\nLeNet详解\r\n李沐——LeNet\r\nimport torch\nfrom torch import nn\n\nclass Reshape(torch.nn.module):\n\t# 用于在通道数不变的情况下调整输入\n\tdef forward(self, x):\n\t\treturn x.view(-1, 1, 28, 28)\n\n\nnet = nn.Sequential(\n\tReshape(),\n\t# input层，输入为1*32*32的图片\n\tnn.Conv2d(1, 6, kernal_size=5, padding=2), nn.Sigmod(),\n\t# C1,6个5*5大小的卷积核，无填充，处理后数据变为28*28*6\n\tnn.AvgPool2d(kernal_size=2, stride=2),\n\t# S2，降采样层，先对2*2的视野进行平均，然后进入激活函数，处理后数据变为14*14*6\n\tnn.Conv2d(6, 16, kernal_size=5), nn.Sigmod(),\n\t# C3，16个大小为5x5的卷积核，步长为1。但是，这一层16个卷积核中只有10个和前面的6层相连接\n\t# 也就是说，这16个卷积核并不是扫描前一层所有的6个通道。而是只扫描其中的三个\n\t# 原因：破图像的对称性，期望学到互补的特征；减少连接的数量\n\tnn.AvgPool2d(kernal_size=2, stride=2),\n\tnn.Flatten(),\n\t# 把数据平铺成一个一维张量\n\tnn.Linear(16 * 5 * 5, 120), nn.Sigmod(),\n\t# 稠密全连接层，将数据逐层处理，直至最终的10类别(手写数字的个数)\n\tnn.Linear(120, 84), nn.Sigmod(),\n\tnn.Linear(84, 10)\n)\r\n随机输入，查看各层的输出\r\nX = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)\nfor layer in net:\n    X = layer(X)\n    print(layer.__class__.__name__,'output shape: \\t',X.shape)\r\n小结\r\n\r\n在卷积神经网络中，我们组合使用卷积层、非线性激活函数和汇聚层\r\n为了构造高性能的卷积神经网络，我们通常对卷积层进行排列，逐渐降低其表示的空间分辨率，同时增加通道数\r\n在传统的卷积神经网络中，卷积块编码得到的表征在输出之前需由一个或多个全连接层进行处理\r\n\r\n","slug":"LeNet","date":"2023-02-20T10:33:27.000Z","categories_index":"AI学习","tags_index":"AI学习","author_index":"碔砆"},{"id":"a0a717998dbf88c81be122d0bce3bca3","title":"信息论","content":"Information Theory\r\nMLAPP 2.8\r\nPreface\r\n信息论(Information\r\nTheory)：应用数学的一个分支，涉及用紧凑的方法来表示数据（如数据压缩和编码），以及具有鲁棒性的储存和传输数据。在机器学习中，信息论常常应用于连续型变量\r\n信息论的基本想法是：一个不太可能发生的事发生了，要比一个非常可能的事件发生，提供更多的信息。尤其是：\r\n\r\n非常可能发生的事件信息量要比较少，且极端条件下，必然事件应该没有信息量\r\n较不可能发生的事件具有更高的信息量\r\n独立事件应具有增量的信息\r\n如：英语中的常见词 a the等要比稀有词短的多\r\n\r\n因此，我们需要一个模型来预测哪种数据是可能的，哪些是不可能的，这就是机器学习领域所关注的问题\r\n为满足上述提到的3个性质，定义一个事件x的自信息(self-information)\r\n为\r\n\r\n定义的单位是奈特，一奈特是以的概率观测到一个事件时获得的信息量，还有其他以2为底数的对数，它们的单位是比特或者香农\r\n熵\r\n自信息只能处理单个的输出，对整个概率分布中的不确定性总量进行量化，我们使用香农熵(Shannon\r\nentropy)\r\n具体的说，对具有K个状态的离散量，其定义如下：\r\n从定义中可以看出：一个分布的香农熵是指遵循这个分布的事件所产生的期望信息总量\r\n它给出了对依据概率分布P生成的符号进行编码所需的比特数在平均意义是的下界\r\n那些接近确定性的分布具有较低的熵，那些接近均匀分布的概率分布具有较高的熵\r\n当x是连续时，香农熵被称为微分熵(differential\r\nentropy)\r\nKL散度\r\n当对于一个随机变量x有两个单独的概率分布，我们使用KL散度(Kullback-Leibler(KL)\r\ndivergence) 来衡量这两个分布的差异\r\n\r\n其中是交叉熵(cross-entropy)，其定义为\r\n\r\n在离散型变量的情况下，KL散度衡量的是，当我们使用一种被设计成能够使得概率分布Q产生的信息的长度最小的编码，发送包含由概率分布P产生的符号的信息时，所需要的额外信息量\r\n交叉熵指，当我们使用模型q时，使用分布为p的数据进行编码所需要的平均比特数来定义编码\r\nKL散度是非负的，当且仅当P和Q在离散型变量的情况下是相同的分布，或者在连续型变量的情况下是”几乎处处“相同的\r\n因为KL散度是非负的并且衡量的是两个分布之间的差异，常被用作分布之间的某种举例，但KL并不是真的距离，它并不对称\r\n互信息\r\n对于两个随机变量X，Y，想知道一个变量能告诉我们多少关于另一个变量的信息。对实值随机变量，我们可以计算这两个随机变量的相关系数。但更一般的方法是确定联合分布与因式分解分布之间的相似性，这就引出了互信息(mutual\r\ninfromation) 的概念，其定义如下：\r\n\r\n当且仅当时，,即当且仅当变量是独立的时候，\r\n参照KL散度的定义，互信息也可写为以下形式：\r\n\r\n其中是条件熵(conditional\r\nentropy)，定义为\r\n\r\n从上式可看出，互信息可解释为在观察到Y之后，X的不确定的减小\r\n与互信息关系密切的一个概念是逐点互信息(pointwise mutual\r\ninformation)，其定义为：\r\n\r\n其衡量了这些事件 一起发生时与各自偶然发生时 之间的差异\r\n连续型随机变量的互信息\r\n","slug":"信息论","date":"2023-02-18T10:32:18.000Z","categories_index":"AI学习","tags_index":"AI学习","author_index":"碔砆"},{"id":"665540b83d48c62e220c6e1d58a9965e","title":"卷积神经网络","content":"卷积神经网络\r\n卷积网络(convolutional network)\r\n又称卷积神经网络(CNN)，是专门用来处理具有类似网格结构的数据的神经网络\r\n通常指那些至少在网络的一层中使用卷积运算来代替一般的矩阵乘法运算的神经网络\r\n卷积运算\r\n卷积是对两个实变函数的一种数学运算\r\n在许多实际问题中，经常需要对所求值沿特定轴进行加权平均，如时间上越近的测量结果越相关\r\n这可以通过一个加权函数实现，表示测量结果距当前时刻的时间间隔，表示是与当前时刻相关的函数\r\n对任意时刻都采取这种加权平均计算，就可以得到一个新的平滑估计函数s：\r\n\r\n称这种运算为卷积(convolution)，通常用星号表示\r\n\r\n在卷积网络的术语中，卷积的第一个参数通常叫做输入(input)\r\n第二个参数叫做核函数(kernel\r\nfunction)，输出有时被称为特征映射(feature\r\nmap)\r\n在机器学习中，输入经常是多维数组的数据，称为张量，核函数通常是由学习算法优化得到的参数\r\n二维的核写作以下形式，卷积是可交换的\r\n\r\n卷积的可交换性的出现是因为我们将核相对输入进行了翻转(flip)\r\n从m增大的角度来看，输入的索引在增大，但是核在减小\r\n许多库会实现另外一个相关函数，称为互相关函数，和卷积运算基本一样，但是没有对核进行翻转\r\n\r\n实际运算步骤为，依据核的大小，在张量上逐行逐列的移动来得到输入\r\n离散卷积可以看作矩阵的乘法，但这个矩阵的元素被限制为必须核另外一些元素相等\r\n如在单变量的离散卷积中，矩阵每一行中的元素都与上一行对应位置平移一个单位的元素相等\r\n这种矩阵叫做Toeplitz矩阵(Toeplitz matrix)\r\n对于二维情况，卷积对应着一个双重分块循环矩阵\r\n除此之外，卷积通常对应着一个非常稀疏的矩阵，这是因为核的大小要远远小于张量的大小\r\n任何一个使用矩阵乘法且不依赖于矩阵结构的特殊性质的神经网络算法都适用于卷积运算\r\ndef corr2d(X, K):\n# 卷积计算\n\th, w = K.shape\n\tY = torch.zeros((X.shape[0] - h + 1), (X.shape[1] - w + 1))\n\tfor i in range(Y.shape[0]):\n\t\tfor j in range(Y.shape[1]):\n\t\t\tY[i, j] = (X[i:i + h, j:j + w] * K).sum()\n\treturn Y\n\nclass Conv2D(nn.module):\n\t\r\npytorch——Conv2d\r\n张闯Pytorch\r\n选择卷积的动机\r\n之前我们已经提过一种名为全连接层的前向传播的神经网络结构，那么又为何要改为选择卷积呢？\r\n主要有以下三个重要的思想：\r\n稀疏交互(sparse interactions)\r\n传统的神经网络使用的是矩阵乘法，这使得每一个输出单元与每一个输入单元都会产生交互\r\n相比之下，卷积网络具有稀疏交互的特征\r\n由于核的大小远小于输入的大小，因此我们可以通过较小的核从较大的图像中提取出一些小的且有意义的特征\r\n这使得需要存储的参数更少，不仅减少了模型的存储需求，还提高了它的统计效率\r\n而在大多数分类问题中，我们不需要关注相隔很远的元素之间的联系，只需要关注邻近的几个元素就足以分类\r\n参数共享(parameter sharing)\r\n参数共享是指在一个模型的多个函数中使用相同的参数\r\n传统的神经网络中，计算一层的输出时，权重矩阵的一个元素只使用一次\r\n而在参数共享下，用于一个输入的权重也会被绑定在其他的权重上，也被称为每一个网络具有绑定的权重\r\n在CNN在，核的每一个元素都作用在输入的每一位置上\r\n这保障了我们在训练阶段也只需要线学习一个参数集合，而无需对每一个位置都学习专门的参数\r\n这使得卷积在存储需求核统计效率方面都有很大的提升\r\n等变表示(equivalen\r\nrepresentations)\r\n首先，先说明分类问题的一个很基础的性质：平移不变性(translation\r\ninvariance)\r\n即不管检测对象出现在图像的哪个位置，神经网络的前面几层对相同的图像应该有相似的反应\r\n而在卷积中，由于参数共享的特殊形式，使得神经网络具有对平移等变的性质\r\n如果一个函数满足输入改变，输出也以同样的方式改变的这一性质，就称为等变\r\n这意味着，检测对象在输入中的改变，仅导致隐藏表示的变化，即\r\n\r\n其中，是权重的重新索引，\r\n又因为要使与，无关，故使得\r\n池化\r\n卷积网络中的一个典型层包括三级：卷积级、探测级、池化级\r\n在卷积级中，这一层并行地计算多个卷积产生一组线性激活响应\r\n在探测级中，每一个线性激活响应将会通过一个非线性的激活函数\r\n在池化级中，我们通过池化函数来进一步调整这一层的输出\r\n池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出\r\n\r\n不管使用怎样的池化函数，当输入做少量平移时，池化能帮助输入的表示近似不变\r\n局部不变性在我们只关心某个特征而不关心其出现的位置时非常有用\r\n使用池化可以看作增加了一个无限强的先验：这一层学得的函数必须具有对少量平移的不变性\r\n因为池化综合了所有邻居的反馈，这使得池化单元少于检测单元成为可能\r\n这种方法可以减少下一层约k倍的输入，从而提高了网络的计算效率\r\n同时，池化对于处理不同大小的输入具有重要作用\r\n如当我们需要对不同大小的图像进行分类时，分类成的输入必须是固定的大小，而这常通过调整池化区域的偏置大小来实现\r\n但与此同时，池化可能会使得一些利用自顶向下信息的神经网络结构变得复杂，如玻尔兹曼机和自编码器\r\n最大池化层和平均池化层\r\n与卷积层类似，汇聚层运算符由一个固定形状的窗口组成，该窗口根据其步幅大小在输入的所有区域上滑动，为固定形状窗口（有时称为汇聚窗口）遍历的每个位置计算一个输出\r\n然而，不同于卷积层中的输入与卷积核之间的互相关计算，汇聚层不包含参数\r\n相反，池运算是确定性的，我们通常计算汇聚窗口中所有元素的最大值或平均值\r\n这些操作分别称为最大汇聚层（maximum\r\npooling）和平均汇聚层（average pooling）\r\n多通道\r\n在处理多通道输入数据时，汇聚层在每个输入通道上单独运算，而不是像卷积层一样在通道上对输入进行汇总\r\n这意味着汇聚层的输出通道数与输入通道数相同\r\n卷积和池化作为一种无限强的先验\r\n先验被认为强或者弱取决于先验中概率密度的集中程度\r\n弱先验具有较高的熵值，这允许数据对于参数的改变具有或多或少的自由性\r\n强先验具有较低的熵值，它在决定参数最终取值时起到更为积极的作用\r\n一个无限强的先验需要对一些参数的概率置零并且完全禁止对这些参数赋值，无论数据对这些参数起到多大的支持\r\n把卷积网络类比成全连接网络，就会对这个全连接网络的权重增加一个无限强的先验\r\n这个先验表示一个隐藏单元的权重必须和它的邻居权重相同，但可以在空间上移动\r\n这个先验也要求除了那些处在隐藏单元的小的空间连续的接受域内的权重外，其他的权重都为零\r\n这个先验说明了该层应该学得的函数只包含局部连接关系并且对平移具有等变性\r\n而从这种类比中，我们可以看出：卷积和池化都可能导致欠拟合\r\n另一个关键是当我们比较卷积模型的统计学习表现时，只能以基准中的其他卷积模型作为比较的对象\r\n基本卷积函数的变体\r\n填充\r\n假设输入形状为，卷积核形状为，那么输出形状将是\r\n因此，卷积的输出形状取决于输入形状和卷积核的形状\r\n而有效的调整输出形状的方式就是 填充(padding)\r\n，以及之后介绍的 步幅(stride)\r\n如上所述，我们在应用多层卷积时，很容易丢失边缘像素，常见的解决方法就是在边界处进行填充\r\n如果我们要填充行列，常见的填充方法是在上下左右各填充1/2\r\n\r\n完成填充操作后，输出形状将变为\r\n以下将介绍三种特别的零填充设置：\r\n\r\n有效卷积：无论如何都不使用零填充的极端情况，卷积核只允许访问图像中那些能够包含整个核的位置\r\n相同卷积：只进行足够的零填充来保持输入和输出具有相同的大小，这种情况会导致输入像素中靠近边界的部分相对于中间部分对于输出像素的影响更小，会导致边界像素存在一定程度的欠表示\r\n全卷积：进行了足够多的零填充，使得每个函数在每个方向上恰好被访问了k次，这种情况下，输出像素靠近边界的部分相比于中间部分是更少像素的函数，导致很难找到一个在卷积映射所有位置表现良好的单核\r\n\r\n通常来讲，零填充的最佳数量在于有效卷积和相同卷积中的某个位置\r\n步幅\r\n在计算互相关时，卷积窗口从输入张量的左上角开始，向下向右滑动，默认每次滑动一个元素\r\n但是，有时候为了高效计算或是缩减采样次数，卷积窗口可以跳过中间位置，每次滑动多个元素\r\n而间隔的像素数就被称为步幅，我们可以对不同方向设置不同的步幅\r\n增加每个方向上相同步幅s后的采样卷积函数变为：\r\n\r\n通常，当垂直步幅为、水平步幅为时，输出形状为\r\n卷积层中的多输入多输出通道\r\n在大多数图像问题中，一张图片由RGB三个通道构成，而之前的讨论中都聚焦于二维张量\r\n因此，我们有必要去在二维张量中增添通道，使之成为三维张量\r\n多输入通道\r\n而当输入具有多个通道时，需要构造一个与输入具有相同通道数的卷积核\r\n当通道数时，我们卷积核的每个输入通道将包含形状为的张量\r\n将这些张量连结在一起可以得到形状为的卷积核\r\n由于输入和卷积核都有个通道，我们可以对每个通道输入的二维张量和卷积核的二维张量进行互相关运算，再对通道求和（将的结果相加）得到二维张量\r\n\r\n这是多通道输入和多输入通道卷积核之间进行二维互相关运算的结果\r\n多输出通道\r\n随着神经网络层数的加深，我们常会增加输出通道的维数，通过减少空间分辨率以获得更大的通道深度\r\n直观地说，我们可以将每个通道看作对不同特征的响应\r\n而现实可能更为复杂一些，因为每个通道不是独立学习的，而是为了共同使用而优化的\r\n因此，多输出通道并不仅是学习多个单通道的检测器\r\n用和分别表示输入和输出通道的数目，为了获得多个通道的输出，我们可以为每个输出通道创建一个形状为\r\n的卷积核张量，这样卷积核的形状是\r\n在互相关运算中，每个输出通道先获取所有输入通道，再以对应该输出通道的卷积核计算出结果\r\n局部连接层\r\n平铺卷积\r\n1\r\n1卷积\r\n","slug":"卷积神经网络","date":"2023-02-18T10:24:15.000Z","categories_index":"AI学习","tags_index":"AI学习","author_index":"碔砆"},{"id":"60fb8c126ebe52ae9e85a9be67ca6bb2","title":"贝叶斯统计","content":"贝叶斯统计\r\nMLAPP 149\r\nDL 85\r\n5.1 Introduction\r\n区别于频率派统计方法和基于估计单一值的方法：基于该估计做所有的预测\r\n另一种方法是在做预测时就考虑所有可能的，后者属于贝叶斯统计(Bayesian\r\nstatistics)\r\n贝叶斯统计的核心在于：使用后验分布来总结我们所知道的关于一组未知变量的一切\r\n频率派的视角是真实参数是未知的定值，而点估计是考虑数据集上函数的随机变量\r\n而贝叶斯统计用概率来反映知识状态的确定性程度，数据集是可观测的，而真实参数是未知或不确定的，因此表示为随机变量\r\n在观察到数据前，我们将的已知知识表示为先验概率分布(prior\r\nprobability distribution) 简称为先验\r\n当我们观测到m个样本后，对第m+1个样本的预测如下：\r\n\r\n可以看出，每个具有正概率密度的都有助于下一个样本的判断，其中贡献有后验密度本身加权\r\n而在观测完数据集D后，若我们仍然不确定的值，那么这个不确定性会直接包含在之后的预测中\r\n当训练数据小时，贝叶斯方法通常泛化的很好，但在训练数据大时，所产生的计算成本很大\r\n5.2 后验分布的总结\r\n5.2.1 MAP估计\r\n通过计算后验均值、中值(median)和模态(mode)我们可以很轻易的得到一个未知量的点估计\r\n通常情况下，后验均值或中值是实值量的最佳选择，后验边缘向量是离散量的最佳选择\r\n然而最受欢迎，也最为常用的后验模型，也就是 最大后验(Maximum A\r\nPosteriori，MAP) 估计\r\nMAP估计将问题简化为了一个优化问题，通过选择后验概率最大的点或是在是连续型的更常见情况下，概率密度最大的点:\r\n\r\n右边的对应着标准的对数似然项，对应着先验分布\r\n尽管MAP估计非常好用，但其并不完善，接下来将介绍一些它的缺点\r\n5.2.1.1 没有不确定度的度量\r\nMAP估计及其他任何点估计都具有一个缺点：缺少一个对不确定度的度量值，这在许多问题中是非常重要的\r\n5.2.1.2\r\n插入MAP估计容易导致过拟合\r\n由于在机器学习中，我们更关注预测的准确性，而不是如何解释模型的参数\r\n但如果不去建模参数中的不确定性，则容易导致模型的预测分布过度自信，从而导致过拟合\r\n5.2.1.3 模态是一个非典型点\r\n选择模态作为后验分布通常是一个很糟糕的选择，因为模态对分布而言通常是非典型的，例子可见MLAPP150F5.1\r\n究其原因是，模态是一个衡量零值的点，而均值和中值考虑了空间的体积\r\n同时，在偏态分布(例子见MLAPP150F5.1)中，也常发生模式为0，但均值不为0的现象\r\n这种分布经常出现在推断方差参数时，尤其是在层次模型中，这使得MAP的表现非常糟糕\r\n当MAP为首的模式不好用时，我们常通过决策理论来总结后验分布\r\n5.2.2 可信区间\r\n5.2.1.1中提出，MAP估计缺少一个对不确定度的度量，而对一个标量参数的置信度的度量就是其后验分布的宽度\r\n这一宽度可以用的\r\n可信区间(credible interval) 来衡量，可以用区间来表示\r\n\r\n这样的区间也许有很多个，因此我们在每一尾中选择质量为的子区间，称其为中心区间(central\r\ninterval)\r\n如果后验分布有一个已知的函数表达式，我们可以通过其分布函数来计算可信区间\r\n\r\n而当我们不知道后验分布的形式时，我们可以从后验分布中选取样本，之后再使用蒙特卡罗近似计算后验分位数\r\n排序个样本，并沿着排序后的列表来找到第的样本，当时，这个样本就成为真正的分位数\r\nTips：可信区间(credible interval)和置信区间(confidence\r\ninterval)要区分开，前者属于贝叶斯统计，后者属于概率统计\r\n5.2.3 在比例差异中的推理\r\n在现实生活中的很常见的例子：商品1有90个好评和10个差评，商品2有2个好评0个差评，该如何选择\r\n对这一问题，我们常通过贝叶斯分析，这一方法也可推广到其他关于比例的情况下\r\n详细过程见MLAPP 155(TO DO)\r\n另一种方法就是通过蒙特卡罗抽样来近似得出二者的后验分布\r\n5.3 贝叶斯模型选择\r\n在一个机器学习问题中，超参数的选择不当常常会引发过拟合或者欠拟合的后果，由此便引出了模型选择这一问题\r\n模型选择的一种方法就是利用机器学习基础2.3节中介绍的交叉验证来估计所选模型的泛化误差\r\n而另一种更有效的方法就是计算所选模型的后验分布\r\n\r\n从中，我们可以很快的计算出MAP模型：，这就是贝叶斯模型选择\r\n如果我们使用一致的先验分布，，则相当于选择最大化的模型\r\n我们称这个量为\r\n边际似然(marginal likelihood)\r\n、微分似然(integrated likelihood) 或\r\n模型m的证据(evidence)\r\n","slug":"贝叶斯统计","date":"2023-02-18T10:23:28.000Z","categories_index":"AI学习","tags_index":"AI学习","author_index":"碔砆"},{"id":"e9525b7ef8e5ead70d9f4aba30aa5c7e","title":"机器学习基础","content":"机器学习基础\r\nDL\r\n1.学习算法\r\n机器学习算法是一种能够从数据中学习的算法，可定义为\r\n”对于某类任务T和性能度量P，一个计算机程序被认为可以从经验E中学习是指，通过经验E改进后，它在任务T上由性能度量P衡量的性能有所提升\"\r\n1.1 任务T\r\n学习的过程本身并不是任务，学习是所谓获取完成任务的能力的过程\r\n通常来讲，机器学习任务被定义为 机器学习系统应该如何处理\r\n样本(example)\r\n样本是指我们从某些希望机器学习系统处理的对象或事件中收集到的已经量化的\r\n特征(feature) 的集合\r\n1.2 性能度量P\r\n为评估机器学习算法的能力，设计其性能的定量度量是有必要的\r\n通常性能度量P是特定于任务T的，如分类型任务，性能度量P为准确率acc\r\n对性能度量P的衡量主要聚焦于未观测数据，即 测试集(test\r\nset) 上\r\n对于很多模型，选择一个与系统理想表现对应的性能度量通常是很有难度的\r\n1.3 经验E\r\n根据学习过程中的不同经验，机器学习算法可以大致分类为\r\n无监督算法 和 有监督算法\r\n二者间主要区别来自于这样的一个视角：老师提供目标y给机器学习系统，指导其应该作什么。在无监督算法中则没有老师这样的角色，算法必须在没有指导的情况下理解数据\r\n2. 容量、过拟合和欠拟合\r\n机器学习的重要目的就是在观测到的数据，即 训练集\r\n上的表现良好之外，也在测试集上表现良好\r\n这之中主要涉及一个算法的 泛化(generalization)\r\n能力\r\n统计学习理论对泛化能力的提升给出了相应的方法：\r\n训练集和测试集数据通过数据集上被称为 数据生成过程(data\r\ngenerating process) 的概率分布生成。而在这一过程中采取\r\n独立同分布(i.i.d assumption)\r\n的假设，对提升泛化能力非常重要\r\n独立同分布指，每个数据集中的样本都是彼此相互独立的，并且训练集和测试集是同分布的\r\n这么做可以使得随机模型的训练误差期望和测试误差期望是一致的\r\n从中，我们可以延申出决定机器学习算法效果是否好的两个因素：\r\n\r\n降低训练误差\r\n缩小训练误差和测试误差的差距\r\n\r\n而这两个因素又分别对应着机器学习的两个主要挑战：欠拟合(underfitting)\r\n和 过拟合(overfitting)\r\n欠拟合指模型不能在训练集上获得足够低的误差，过拟合指训练误差和测试误差之间的差距过大\r\n由此又另外延申出一个模型的概念：容量(capacity)\r\n容量指一个模型拟合各种函数的能力，容量过高则会使模型记住了不适于测试集的训练集性质从而表现为过拟合\r\n一种常见的控制容量的方法是选择 假设空间(hypothesis\r\nspace) ，即学习算法的选择范围为解决方法的函数集\r\n主要控制容量的方法是：改变输入特征的数目和加入这些特征对应的参数\r\n而在这一过程中，应当遵循 奥卡姆剃刀原理\r\n对于容量任意高的极端情况，则归于 非参数模型\r\n的概念\r\n非参数模型，指模型的参数会随着输入特征的变多而变多\r\n与参数模型相比，非参数模型更为灵活，所需要的假设更少，但一个严重的问题在于对于大部分数据而言计算困难\r\n2.1 没有免费午餐定理\r\n对于不同的问题，通常需要设计不同的模型，而学习理论也表明机器学习算法可以在有限个训练集样本中很好地泛化\r\n但是，必须强调，不存在万能的最佳模型，即 没有免费午餐定理(no\r\nfree lunch theorem)\r\n这一理论表明，在所有可能的数据生成分布上平均之后，每一个分类算法在未事先观测的点上具有相同的错误率\r\n但必须强调，这一定理仅在考虑所有可能的数据生成分布时才成立\r\n在实际应用中，我们常对所遇到的概率分布进行假设，并针对该假设设计表现良好的算法\r\n2.2 正则化(TO DO)\r\n正则化(regularization)\r\n是指修改学习算法，使其降低泛化误差而非训练误差\r\n2.3 超参数和验证集\r\n超参数(hyper-parameter)\r\n常被设置用以控制算法的行为，超参数不是通过学习算法本身学习出来的\r\n常见的设置超参数的原因是该参数不适合在训练集上学习，如控制模型容量的所有超参数\r\n如果在训练集上训练这些超参数，这些超参数总是会区域最大可能的模型容量从而导致过拟合\r\n为解决这个问题，引出一个训练算法观测不到的 验证集(validation\r\nset) 样本\r\n验证集重要用于挑选模型的超参数\r\n验证集的重点在于测试样本不能以任何形式参与到模型的选择之中，包括设置超参数\r\n也是因此，之前提到的测试集中的样本不能用于验证集\r\n通常情况下，经常将用于学习参数的训练集中挑选子集来构建验证集，比例常见为4：1\r\n2.3.1 交叉验证\r\n在训练中，若训练集的误差很小，在数据集太小时，可能会带来一些问题\r\n一个小规模的测试集意味着平均测试误差估计的统计不确定性，使得很难判断算法A、B之间在给定任务上的优劣\r\n常见的一个解决方法为k-fold交叉验证\r\n算法如下：将训练集分为k个互斥子集，训练除第k个子集外的每一个子集，并在第k个子集上进行测试，以这种方式进行滚动式循环训练。之后，计算所有子集的平均误差，并以此来代替测试误差\r\n（tips：每个点只用于一次测试，和n-1次训练）\r\n2.4 估计、偏差和方差（TO DO）\r\n2.4.1 点估计\r\n2.4.2 偏差\r\n2.4.3 方差和标准差\r\n2.4.4\r\n权衡偏差和方差以最小化均分误差\r\n2.4.5 一致性\r\n2.5 最大似然估计\r\nMLAPP 217 估计统计模型参数的一种常见方法是计算其最大似然估计(Maximum\r\nlikelihood estimation，MLE) 对的最大似然估计定义为：\r\n\r\n为简便运算，常将原函数进行取对数处理\r\n在实际进行代码运行时，我们更经常等价地采取最小化负对数的似然\r\n一种解释最大似然估计的观点是将它看作最小化训练集上的经验分布和数据之间的差异，二者间的差异程度由KL散度度量\r\n\r\n其中左边一项仅涉及数据生成过程，因此只需要最小化\r\n2.5.1 条件对数似然和均方误差（to\r\ndo）\r\n由之前最大似然估计的定义，可进一步拓展到估计条件概率从而给定x预测y\r\n而这就构成了大多数监督学习的基础\r\n2.5.2 最大似然的性质\r\n最大似然估计的最重要的一点在于，它被证明了当样本数目时，就收敛率而言是最好的渐进估计\r\n在以下条件下，最大似然估计具有一致性\r\n\r\n真实分布必须在模型族中，否则，没有估计可以还原\r\n真实分布必须刚好对应一个值，否则，最大似然估计恢复除真实分布后，也不能决定数据生成过程使用哪个\r\n\r\n除了最大似然估计之外，还有其他的归纳准则，且都具有一致性\r\n但这些一致估计的 统计效率(statistic efficiency)\r\n可能区别很大\r\n在综合考虑了一致性和统计效率之后，最大似然通常是机器学习中的首选估计方法\r\n2.6 贝叶斯统计\r\n2.7 监督学习算法\r\n2.8 无监督学习算法\r\n2.9 随机梯度下降\r\n","slug":"机器学习基础","date":"2023-02-18T08:33:33.000Z","categories_index":"AI学习","tags_index":"AI学习","author_index":"碔砆"},{"id":"563a71603f1af2b0a426978adee1756b","title":"二叉树","content":"二叉树\r\n树\r\n定义\r\n一棵树t是一个非空的有限元素的集合，其中一个元素为根（root），其余元素组成t的子树（subtree）\r\n在层次数据中最高层的元素是根，其直接下一级元素是子树的根\r\n树的画法\r\n在画一棵树时，每一个元素都代表一个节点。树根画在最上方，其子树画在下面。在根和子树的根之间有一条边。子树结构相同。\r\n在树中没有子元素的元素称为叶（leaf）\r\n术语\r\n级 ：树根是1级，子节点是父节点级树+1\r\n高度（深度） ：树中级的个数\r\n度：一个结点的子节点个数，一棵树的度是其元素的度的最大值\r\n二叉树\r\n定义\r\n一棵二叉树（binary tree）\r\nt是有限个元素的集合（可以为空）。当二叉树非空时，其中有一个元素称为根，余下的元素被划分为两颗二叉树，分别称为t的左子树和右子树。\r\n和树的根本区别\r\n\r\n二叉树的每个元素都恰好有两颗子树。而树的每个元素可有任意数量的子树\r\n在二叉树中，每个元素的子树都是有序的，有左子树和右子树之分。而树的子树是无序的\r\n\r\n二叉树的特性\r\n特性1 一棵二叉树有n个元素，n&gt;0，它有n-1条边\r\n特性2\r\n一棵二叉树的高度为h，h&gt;=0，它最少有h个元素，最多有2h\r\n-1个元素\r\n特性3\r\n一棵二叉树有n个元素，n&gt;0，它的高度最大为n，最小高度为[log2(n+1)]\r\n特性4\r\n一棵非空二叉树的第i层上至多有2i-1个节点\r\n满二叉树 当高度为h的二叉树恰好有2h\r\n-1个元素时，称其为满二叉树。\r\n完全二叉树\r\n对高度为h的满二叉树的元素，从第一层到最后一层，在每一次中从左至右，顺序编号，从1到2h\r\n-1。从中删除k个其编号为\r\n2h-i元素，1&lt;=i&lt;=k&lt;2h,所得到的二叉树被称为完全二叉树，即按从上到下，从左到右的顺序编号，号码与满二叉树一致\r\n特性5\r\n设完全二叉树的一元素其编号为i，1&lt;=i&lt;=n。有以下关系成立\r\n\r\n如果i=1，则该元素为二叉树的根。若i&gt;1，则其父节点的编号为 i/2\r\n如果2i&gt;n，则该元素无左孩子。否则，其左孩子的编号的编号为2i\r\n如果2i+1&gt;n，则该元素无右孩子。否证，其右孩子的编号为 2i+1\r\n\r\n二叉树的描述\r\n数组描述：按照编号存储在数组的相应位置里。\r\n仅在缺少的元素数目比较少时适用\r\n链表描述：\r\ntemplate &lt;class T&gt;\nstruct binaryTreeNode\n&#123;\n    T element;\n    binaryTreeNode&lt;T&gt; *leftChild,*rightChild;\n    binaryTreeNode() &#123;leftChild &#x3D; rightChild &#x3D; NULL;&#125;\n    binaryTreeNode(const T&amp; theElement,\n                   binaryTreeNode *theLeftChild &#x3D; NULL,\n                   binaryTreeNode *theRightChild &#x3D; NULL)\n    &#123;\n        element(theElement)\n        leftChild &#x3D; theLeftChild;\n        rightChild &#x3D; theRightChild;\n\t&#125;\n&#125;\r\n二叉树的操作\r\n前序遍历\r\ntemplate &lt;class T&gt;\nvoid preOrder(binaryTreeNode&lt;T&gt; *t)\n&#123;\n    if(t !&#x3D; NULL)\n    &#123;\n        cout&lt;&lt;t-&gt;element;\n        preOrder(t-&gt;leftChild);\n        preOrder(t-&gt;rightChild);\n\t&#125;\n&#125;\r\n中序遍历\r\ntemplate &lt;class T&gt;\nvoid inOrder(binaryTreeNode&lt;T&gt; *t)\n&#123;\n    if(t !&#x3D; NULL)\n    &#123;\n        inOrder(t-&gt;leftChild);\n        cout&lt;&lt;t-&gt;element;\n        inOrder(t-&gt;rightChild);\n\t&#125;\n&#125;\r\n后序遍历\r\ntemplate &lt;class T&gt;\nvoid postOrder(binaryTreeNode&lt;T&gt; *t)\n&#123;\n    if(t !&#x3D; NULL)\n    &#123;\n        postOrder(t-&gt;leftChild);\n        postOrder(t-&gt;rightChild);\n        cout&lt;&lt;t-&gt;element;\n\t&#125;\n&#125;\r\n层序遍历\r\ntemplate &lt;class T&gt;\nvoid levelOrder(binaryTreeNode&lt;T&gt; *t)\n&#123;\n    queue&lt;binaryTreeNode&lt;T&gt;*&gt; q;\n    while(t !&#x3D; NULL)\n    &#123;\n        q.push(t);\n        if(t-&gt;leftChild !&#x3D; NULL)\n            q.push(t-&gt;leftChild);\n        if(t-&gt;rightChild !&#x3D; NULL)\n            q.push(t-&gt;rightChild);\n        try &#123;t &#x3D; q.front();&#125;\n        catch (queueEmpty) &#123;return;&#125;\n        q.pop();\n    &#125;\n&#125;\r\n","slug":"二叉树","date":"2022-04-20T01:49:36.000Z","categories_index":"数据结构","tags_index":"数据结构","author_index":"碔砆"},{"id":"b96df039eaa03d2a5634c2cc83e5cad0","title":"线性表","content":"\r\n","slug":"线性表","date":"2022-04-20T01:41:09.000Z","categories_index":"","tags_index":"","author_index":"碔砆"},{"id":"9fc7aaa29955e8777b63cdc5e2f3c111","title":"快速幂","content":"快速幂\r\n快速幂——OIWiki\r\n在O(logn)的时间内计算 an\r\n可以运用于模意义下取幂、矩阵幂等运算\r\n算法描述\r\n二进制取幂的想法是，我们将取幂的任务按照指数的 二进制表示\r\n来分割成更小的任务。\r\n因为有个二进制位，因此当我们知道了后，我们只用计算O(logn)次乘法就可以计算出\r\n中任意一个元素都是前一个元素的平方\r\n算法实现\r\nlong long binpow(long long a, long long b) {\n  if (b == 0) return 1;\n  long long res = binpow(a, b / 2);\n  if (b % 2)\n    return res * res * a;\n  else\n    return res * res;\n}\r\n矩阵快速幂\r\n基本公式：\r\n由的递推公式决定\r\n实现方法\r\nstruct Matrix\n{\n    long long m[15][15]={0};\n    Matrix operator* (const Matrix &amp;b) const\n    {\n        Matrix res;\n        for (int i = 1; i &lt;= n; i++)\n            for (int j = 1; j &lt;= n; j++)\n                for (int k = 1; k &lt;= n; k++)\n                    res.m[i][j] = (res.m[i][j] + (m[i][k] * b.m[k][j])%mod) % mod;\n    return res;\n    }\n};\nMatrix qpow(Matrix base,int b){      //ans应初始化为单位矩阵\n    Matrix ans;\n    for(int i=1;i&lt;=n;i++)\n        ans.m[i][i]=1;\n    while (b) {\n        if (b &amp; 1) ans = ans * base;\n        base = base * base;\n        b &gt;&gt;= 1;\n    }\n    return ans;\n}\r\n最后的答案根据题目另外写\r\n","slug":"快速幂","date":"2022-02-14T03:22:15.000Z","categories_index":"算法","tags_index":"算法","author_index":"碔砆"},{"id":"4aa58050e03a812b4fa7990ab0778c08","title":"二分匹配","content":"二分匹配\r\nbool dfs(int x)\n&#123;\n    for(int i&#x3D;1;i&lt;&#x3D;n;i++)\n    &#123;\n        if(mapp[x][i]&amp;&amp;(!used[i]))\n        &#123;\n            used[i]&#x3D;1;\n            if(linker[i]&#x3D;&#x3D;-1||dfs(linker[i]))\n            &#123;\n                linker[i]&#x3D;x;\n                return true;\n            &#125;\n        &#125;\n    &#125;\n    return false;\n&#125;\nint hungary()\n&#123;\n    int ans&#x3D;0;\n    memset(linker,-1,sizeof(linker));\n    for(int i&#x3D;1;i&lt;&#x3D;n;i++)\n    &#123;\n        memset(used,0,sizeof(used));\n        if(dfs(i)) ans++;\n    &#125;\n    return ans;\n&#125;\r\n","slug":"二分匹配","date":"2022-01-17T08:44:06.000Z","categories_index":"算法","tags_index":"算法","author_index":"碔砆"},{"id":"8fd51d91ee8f082030932013cc236c76","title":"图的存储","content":"图的存储\r\n直接存边\r\n使用一个数组来存边，数组中的每个元素都包含一条边的起点与终点\r\nstruct edge&#123;\n    int n,w;\n&#125;\nvector&lt;edge&gt; e;\nvector&lt;bool&gt; vis\nvoid dfs(int u) &#123;\n    if (vis[u]) return;\n    vis[u] &#x3D; true;\n    for (int i &#x3D; 1; i &lt;&#x3D; m; ++i) \n        if (e[i].u &#x3D;&#x3D; u)\n            dfs(e[i].v);\n&#125;\r\n遍历效率低下，一般仅用于Kruskal算法\r\n邻接矩阵\r\n使用一个二维数组adj来存边，其中adj[u][v]为1表示存在u到v的边，为0表示不存在\r\n如果是带边权的图，可以在 adj[u][v] 中存储\r\n一般只在稠密图中使用\r\n邻接表\r\n使用一个支持动态增加元素的数据结构构成的数组，如vector&lt;int&gt; adj[n + 1]来存边\r\n其中adj[u]存储的是点u的所有出边的相关信息（终点、边权等）\r\nvector&lt;bool&gt; vis;\nvector&lt;vector&lt;int&gt; &gt; adj;\n\nbool find_edge(int u, int v) &#123;\n  for (int i &#x3D; 0; i &lt; adj[u].size(); ++i)\n    if (adj[u][i] &#x3D;&#x3D; v)\n      return true;\n  return false;\n&#125;\n\nvoid dfs(int u) &#123;\n  if (vis[u]) return;\n  vis[u] &#x3D; true;\n  for (int i &#x3D; 0; i &lt; adj[u].size(); ++i) \n    dfs(adj[u][i]);\n&#125;\n    adj[u].push_back(v);    &#x2F;&#x2F;插入边\r\n存各种图都很合适，尤其适用于需要对一个点的所有出边进行排序的场合\r\n前向星\r\nstruct node&#123;\n\tint x,y,w,next;\n&#125;a[500005];\n\nint len,last[15000]; &#x2F;&#x2F;len边的索引，last 点所连最后一条边的索引\n\nvoid ins(int x,int y,int w)&#123;\n\tlen++;\n\ta[len].x&#x3D;x;a[len].y&#x3D;y;a[len].w&#x3D;w;\n\ta[len].next&#x3D;last[x];last[x]&#x3D;len;\n&#125;\n\nfor(int i&#x3D;last[x];i;i&#x3D;a[i].next) &#x2F;&#x2F;遍历\r\n","slug":"图的存储","date":"2022-01-16T08:23:46.000Z","categories_index":"算法","tags_index":"算法","author_index":"碔砆"},{"id":"530e1936c4d6e837f2bf21a86f73c106","title":"最短路","content":"最短路\r\n性质\r\n对于边权为正的图，任意两个结点之间的最短路，不会经过重复的结点\r\n对于边权为正的图，任意两个结点之间的最短路，不会经过重复的边\r\n对于边权为正的图，任意两个结点之间的最短路，任意一条的结点数不会超过n，边数不会超过n-1\r\n记号\r\nn为图上点的数目，m为图上边的数目\r\ns为最短路的源点\r\nD(u)为s点到u点的实际最短路长度\r\ndis(u)为s点到u点的估计最短路长度\r\nw(u,v)为(u,v)这一条边的边权\r\nFloyd算法\r\n","slug":"最短路","date":"2022-01-16T07:42:55.000Z","categories_index":"算法","tags_index":"算法","author_index":"碔砆"},{"id":"48433a70b7e09e957d48f01fa1012852","title":"STL","content":"STL\r\nSTL——OIWiki\r\n共同点\r\n\r\n声明形式： 容器名&lt;数据类型&gt; 变量名\r\n迭代器：用来访问和检查STL容器中的元素的对象，与数据指针类似。主要支持自增（++）和解引用（*）运算符，其中自增用来移动迭代器，解引用可以获取或修改它指向的元素。\r\n用法：容器名&lt;数据类型&gt;::iterator（可用auto代替）\r\n共有函数\r\n= 赋值运算符以及复制构造函数\r\nbegin() 返回指向开头元素的迭代器\r\nend()\r\n返回指向末尾的下一个元素的迭代器。\r\nsize() 返回容器的元素个数\r\nmax_size() 返回容器理论上能储存最大的元素个数\r\nempty() 返回元素是否为空\r\nswap() 交换两个容器\r\nclear() 清空容器\r\n比较运算符 按字典序比较两个元素的大小\r\n\r\n序列式容器\r\nvector\r\nstd::vector 是STL提供的内存连续的，可变长度的数组。\r\n能提供线性复杂度的插入和删除，以及常数复杂度的随机访问\r\n优点\r\n\r\n可以动态分配内存\r\n重写了比较运算符和赋值运算符\r\n初始化简单，可以&#123; &#125;或者=\r\n\r\n初始化操作\r\nvector&lt;数据类型&gt; 变量名(初始空间，初始值)\r\n创建拷贝\r\nvector&lt;数据类型&gt; 变量名(被拷贝的变量名)\r\n移动整个容器\r\nvector&lt;数据类型&gt; 变量名(std::move(原容器))\r\n成员函数\r\nat(pos),operator[pos] 返回下标为pos的元素\r\nfront() 返回首元素的引用\r\nback() 返回末尾元素的引用\r\ndata() 返回数组第一个元素的指针\r\nresize() 改变vector的长度，多退少补\r\nreserve()\r\n使得vector预留一定内存空间，避免不必要的内存拷贝\r\ninsert() 支持在某个迭代器位置插入元素，线性复杂度\r\nerase()\r\n删除某个迭代其或者区间的元素，返回最后被删除的迭代器\r\npush_back() 在末尾插入一个元素\r\npop_back() 删除末尾元素\r\n### array\r\nstd::array是STL提供的内存连续、长度固定的数组数据结构,本质是对原生数组的直接封装\r\n#### 成员函数\r\nat(pos),operator[pos] 返回下标为pos的元素\r\nfront() 返回首元素的引用\r\nback() 返回末尾元素的引用\r\ndata() 返回数组第一个元素的指针\r\nfill(指定值) 以指定值填充容器\r\ndeque\r\nstd::deque是STL提供的双端队列数据结构\r\n成员函数\r\n同vector\r\npush_front() 在头部插入一个元素\r\npop_front() 删除头部元素\r\nlist\r\nstd::list 是STL提供的一个双向链表数据结构\r\n成员函数\r\n同deque\r\n关联式容器\r\nset\r\nset是关联容器，含有键值类型对象的已排序集\r\n内部通常采用红黑树实现\r\n插入与删除操作\r\ninsert(x) 当容器中没有等价元素的时候，将元素 x 插入到\r\nset 中。\r\nerase(x) 删除值为 x 的 所有\r\n元素，返回删除元素的个数。\r\nerase(pos) 删除迭代器为 pos\r\n的元素，要求迭代器必须合法。\r\nerase(first,last) 删除迭代器在\\([First,last)\\)范围内的所有元素。\r\n迭代器\r\nbegin() 返回指向首元素的迭代器，其中\r\n*begin = front\r\nend()\r\n返回指向数组尾端占位符的迭代器，注意是没有元素的。\r\nrbegin()\r\n返回指向逆向数组的首元素的逆向迭代器，可以理解为正向容器的末元素。\r\nrend()\r\n返回指向逆向数组末元素后一位置的迭代器，对应容器首的前一个位置，没有元素。\r\n查找操作\r\ncount(x) 返回键值为x的元素数量\r\nfind(x) 存在键值为x的元素时会返回该元素的迭代器\r\nlower_bound(x)\r\n返回指向首个不小于给定键的元素的迭代器。如果不存在这样的元素，返回\r\nend()\r\nupper_bound(x)\r\n返回指向首个大于给定键的元素的迭代器。如果不存在这样的元素，返回end()\r\nmap\r\nmap是有序键值对容器，它的元素的键是唯一的。map通常实现为红黑树\r\n搜索、移除和插入操作拥有对数复杂度\r\nmap&lt;Key,T&gt; mp #### 操作\r\n查询操作同set\r\n操作与删除操作同set\r\n可以直接通过下标访问来进行查询或插入操作。例如\r\nmp[\"Alan\"]=100\r\n通过向 map\r\n中插入一个类型为pair&lt;Key, T&gt;的值可以达到插入元素的目的，例如mp.insert(pair&lt;string,int&gt;(\"Alan\",100))\r\n容器适配器\r\n栈\r\nstd::stack是一种FILO的容器适配器\r\n操作\r\ntop() 访问栈顶元素\r\npop() 删除栈顶元素\r\npush(x) 向栈顶插入x元素\r\nempty() 询问容器是否为空\r\nsize() 查询容器中的元素数量\r\n队列\r\nstd::queue是一种FIFO的容器适配器\r\n操作\r\n同stack\r\nfront() 访问队首元素\r\n优先队列\r\nstd::priority_queue\r\n操作\r\n同queue\r\n其他非STL容器\r\nbitset\r\nstring\r\npair\r\nstd::pair\r\n是标准库中定义的一个类模板。用于将两个变量关联在一起，组成一个“对”，而且两个变量的数据类型可以是不同的。\r\n操作\r\n初始化\r\n可以在定义时直接完成pair的初始化\r\n也可以使用先定义，后赋值的方法完成pair的初始化\r\n还可以使用std::make_pair函数。\r\n该函数接受两个变量，并返回由这两个变量组成的pair\r\n一种常用的方法是使用宏定义#define mp make_pair，将有些冗长的make_pair化简为mp\r\nmake_pair可以配合auto使用，以避免显式声明数据类型\r\n访问\r\n通过成员函数first与second,可以访问pair中包含的两个变量\r\n比较\r\n&lt;、&gt;、&lt;=、&gt;=\r\n四个运算符会先比较两个pair中的第一个变量，在第一个变量相等的情况下再比较第二个变量\r\n算法部分\r\nnext_permutation\r\n通常用于生成序列的全排列\r\n","slug":"STL","date":"2022-01-15T12:53:20.000Z","categories_index":"算法","tags_index":"算法","author_index":"碔砆"},{"id":"4421686b118d9483ed6838244ba67f35","title":"背包DP","content":"背包DP\r\n背包DP——OIWiki\r\n0-1背包\r\n已知条件有第个物品的重量，价值，以及背包的总容量,每个物体只有取和不取两种状态\r\n设 DP 状态为在只能放前个物品的情况下，容量为的背包所能达到的最大总价值\r\n状态转移方程\r\n\r\n枚举时，从枚举到，保证总是在前被更新\r\nfor (int i = 1; i &lt;= n; i++)\n    for (int l = W; l &gt;= w[i]; l--)  \n        f[l]=max(f[l], f[l - w[i]] + v[i]);\r\n多种价值时开多维数组即可\r\n枚举时也多开一层循环 ## 完全背包 完全背包模型与 0-1 背包类似，与 0-1\r\n背包的区别仅在于一个物品可以选取无限次，而非仅能选取一次\r\n状态转移方程\r\n\r\n枚举时，从枚举到\r\n\r\ntips: 注意f[0]的初始化，保证f[0]=0\r\n\r\n多重背包\r\n多重背包也是 0-1 背包的一个变式。与 0-1 背包的区别在于每种物品有个，而非一个\r\n朴素想法：将每种物品的个，视为不同种物品\r\n二进制分组优化\r\n用代表第i种物品拆分出的第j个物品\r\n朴素法中存在问题:重复考虑了「同时选」与「同时选」这两个完全等效的情况\r\n通过二进制分组的方式使拆分方式更加优美\r\n具体地说就是令分别表示由个单个物品「捆绑」而成的大物品。特殊地，若不是2的整数次幂，则需要在最后添加一个由个单个物品「捆绑」而成的大物品用于补足\r\nindex = 0;\nfor (int i = 1; i &lt;= m; i++) {\n    int c = 1, p, h, k;     //c表示2进制\n    cin &gt;&gt; p &gt;&gt; h &gt;&gt; k;     //p价值，h体积，k数量\n    while (k - c &gt; 0) {\n        k -= c;\n        list[++index].w = c * p;\n        list[index].v = c * h;\n        c *= 2;\n    }\n  list[++index].w = p * k;  //补足\n  list[index].v = h * k;\n}\r\n混合背包\r\n将以上3种背包结合，分开求解即可\r\n\r\n","slug":"背包DP","date":"2022-01-15T07:21:57.000Z","categories_index":"算法","tags_index":"算法","author_index":"碔砆"},{"id":"f6af5bf3fb5f66f52237908153c03aea","title":"动态规划","content":"动态规划DP\r\n动态规划——OIWiki\r\n思路\r\n应用于子问题重叠的情况，即不同的子问题拥有公共的子问题\r\n（子问题的求解是递归进行的，将其划分为更小的子子问题）\r\n对于每个子子问题只求解一次，并将其保存在一个表格中。 ####\r\n最优子结构性质\r\n问题的最优解由相关子问题的最优解组合而成，而这些子问题可以独立求解 ###\r\n基础步骤 - 刻画最优解的结构特征（思考最优解的形式） -\r\n尝试递归的定义最优解的值（即考虑从\\(i-1\\)转移到\\(i\\)） - 计算最优解 -\r\n利用计算出的信息构造最优解\r\n实现方法\r\n\r\n带备忘的自顶向下法\r\n按自然递归形式编写，在过程中保存各个子问题的解\r\n自底向上法 将子问题按规模由从小到大的顺序求解\r\n\r\n","slug":"动态规划","date":"2022-01-15T01:55:19.000Z","categories_index":"算法","tags_index":"算法","author_index":"碔砆"},{"id":"c18469ab20e7b093d36b4a61728d07eb","title":"并查集","content":"并查集\r\n并查集——OIwiki\r\n并查集——知乎\r\n种类并查集——知乎\r\n应用\r\n\r\n并查集判环\r\n\r\n","slug":"并查集","date":"2022-01-13T07:25:39.000Z","categories_index":"算法","tags_index":"算法","author_index":"碔砆"},{"id":"827701aa0aeba949971d70c354c78372","title":"贪心算法","content":"贪心\r\n基本思路\r\n每一步行动总是采取最优解\r\n适用范围\r\n贪心算法在有最优子结构的问题中尤为有效。\r\n最优子结构的意思是问题能够分解成子问题来解决，子问题的最优解能递推到最终问题的最优解\r\n证明方法\r\n\r\n微扰(邻项交换)：证明再任意局面下，任何对局部最优策略的微小改变都会造成整体结果变差。(常用于以“排序”为贪心策略的证明)\r\n范围缩放：证明任何对局部最优策略作用范围的扩展都不会造成整体结果变差\r\n决策包容性：证明在任意局面下，做出局部最优决策后，在问题状态空间的可达集合包含了作出其他任何决策后的可达集合。换言之，这个局部最优决策提供的可能性包含其他所有策略提供的可能性。\r\n反证法：交换方案中任意两个元素，如果答案没变好，贪心即最优解\r\n归纳法：先找出边界情况的最优解F1，然后再证明：对每个n，Fn+1都能由Fn推出\r\n\r\n常见题型\r\n\r\n「我们将 XXX\r\n按照某某顺序排序，然后按某种顺序（例如从小到大）选择。」\r\n「我们每次都取 XXX 中最大/小的东西，并更新 XXX」（有时「XXX\r\n中最大/小的东西」可以优化，比如用优先队列维护）\r\n背包相关问题：给出n个物体，第i个物体重量为wi。选择尽量多的物体，使得总重量不超过C\r\n区间相关问题：数轴上有n个开区间(ai,bi)。选择尽量多区间，使这些区间两两没有公共点\r\nHuffman编码：给出n个字符的频率ci,给每个字符赋予一个01编码串，使得任意一个字符的编码不是另一个字符编码的前缀，而且编码后的总长度尽量小\r\n\r\n例题\r\n区间相关问题 1. 按右端点排序，取与之前不冲突的区间 2.\r\n求最大相交区间\r\n","slug":"贪心算法","date":"2022-01-13T02:24:15.000Z","categories_index":"算法","tags_index":"算法","author_index":"碔砆"},{"id":"f39f5057e79f1bc6b78f8eec0e6063d5","title":"最小公倍数LCM","content":"最小公倍数LCM\r\n原理\r\n最大公约数法\r\n最小公倍数 = 两整数的乘积 ➗ 最大公倍数\r\n前置知识——最大公因数GCD\r\n","slug":"最小公倍数LCM","date":"2022-01-12T10:29:18.000Z","categories_index":"算法","tags_index":"算法","author_index":"碔砆"}]