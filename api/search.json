[{"id":"139386a48ee9ff8eae54fecf132447a0","title":"LeNet","content":"LeNet\r\nIntroduction\r\n这是世界上最早发布的卷积神经网络之一，由AT&amp;T贝尔实验室的研究员Yann\r\nLeCun在1989年提出的（并以其命名）\r\n目的是识别图像 MNIST 中的手写数字\r\n当时，Yann\r\nLeCun发表了第一篇通过反向传播成功训练卷积神经网络的研究，这项工作代表了十多年来神经网络研究开发的成果\r\n模型结构\r\n整体上看，LeNet主要分为两个部分： - 卷积编码器：由两个卷积层组成 -\r\n全连接层密集块：由三个全连接层组成\r\n\r\n每个卷积块中的基本单元是一个卷积层、一个sigmoid激活函数和平均汇聚层\r\n每个卷积层使用卷积核和一个sigmoid激活函数\r\n这些层将输入映射到多个二维特征输出，通常同时增加通道的数量\r\n为了将卷积块的输出传递给稠密块，我们必须在小批量中展平每个样本\r\n换言之，我们将这个四维输入转换成全连接层所期望的二维输入\r\n这里的二维表示的第一个维度索引小批量中的样本，第二个维度给出每个样本的平面向量表示\r\nLeNet的稠密块有三个全连接层，分别有120、84和10个输出\r\n代码\r\nLeNet详解\r\n李沐——LeNet\r\nimport torch\nfrom torch import nn\n\nclass Reshape(torch.nn.module):\n\t# 用于在通道数不变的情况下调整输入\n\tdef forward(self, x):\n\t\treturn x.view(-1, 1, 28, 28)\n\n\nnet = nn.Sequential(\n\tReshape(),\n\t# input层，输入为1*32*32的图片\n\tnn.Conv2d(1, 6, kernal_size=5, padding=2), nn.Sigmod(),\n\t# C1,6个5*5大小的卷积核，无填充，处理后数据变为28*28*6\n\tnn.AvgPool2d(kernal_size=2, stride=2),\n\t# S2，降采样层，先对2*2的视野进行平均，然后进入激活函数，处理后数据变为14*14*6\n\tnn.Conv2d(6, 16, kernal_size=5), nn.Sigmod(),\n\t# C3，16个大小为5x5的卷积核，步长为1。但是，这一层16个卷积核中只有10个和前面的6层相连接\n\t# 也就是说，这16个卷积核并不是扫描前一层所有的6个通道。而是只扫描其中的三个\n\t# 原因：破图像的对称性，期望学到互补的特征；减少连接的数量\n\tnn.AvgPool2d(kernal_size=2, stride=2),\n\tnn.Flatten(),\n\t# 把数据平铺成一个一维张量\n\tnn.Linear(16 * 5 * 5, 120), nn.Sigmod(),\n\t# 稠密全连接层，将数据逐层处理，直至最终的10类别(手写数字的个数)\n\tnn.Linear(120, 84), nn.Sigmod(),\n\tnn.Linear(84, 10)\n)\r\n随机输入，查看各层的输出\r\nX = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)\nfor layer in net:\n    X = layer(X)\n    print(layer.__class__.__name__,'output shape: \\t',X.shape)\r\n小结\r\n\r\n在卷积神经网络中，我们组合使用卷积层、非线性激活函数和汇聚层\r\n为了构造高性能的卷积神经网络，我们通常对卷积层进行排列，逐渐降低其表示的空间分辨率，同时增加通道数\r\n在传统的卷积神经网络中，卷积块编码得到的表征在输出之前需由一个或多个全连接层进行处理\r\n\r\n","slug":"LeNet","date":"2023-02-20T10:33:27.000Z","categories_index":"AI学习","tags_index":"AI学习","author_index":"碔砆"},{"id":"9d8d702581fafe91f0e84d17245c7b9e","title":"最大公因数GCD","content":"最大公因数GCD\r\n原理\r\n利用了欧几里得算法，即辗转相除法\r\n核心等式——gcd(a,b) = gcd(b,a mod b)\r\n证明\r\na可以表示成a = kb + r（a，b，k，r皆为正整数)\r\n假设d是a,b的一个公约数，记作d|a,d|b) 即a和b都可以被d整除。\r\n而r = a - kb，两边同时除以d\r\nr/d=a/d-kb/d，由等式右边可知m=r/d为整数，因此d|r\r\n因此d也是b,a mod b的公约数。\r\n因(a,b)和(b,a mod b)的公约数相等，则其最大公约数也相等，得证。\r\n算法实现\r\n\r\n简单方法 int gcd(int m,int n)\n&#123;    \n    int t,r;    \n    if (m&lt;n)        //为了确保是大数除小数    \n    &#123;        \n        t=m;        \n        m=n;       \n        n=t;    \n    &#125;    \n \n    while((m%n)!=0) //辗转相除    \n    &#123;        \n        r=m%n;        \n        m=n;        \n        n=r;    \n    &#125;   \n \n    return n;\n&#125;\r\n递归方法 int gcd(int x, int y)\n&#123;\tif (y)\t\t\t\n            return gcd(y, x%y);\t\t\n        else\t\t\t\n            return x;\n&#125;\r\n位运算算法 int gcd(int x, int y)\n&#123;\n    while(y^=x^=y^=x%=y);\n    return x;\n&#125;\r\n\r\n","slug":"最大公因数GCD","date":"2023-02-18T10:47:55.939Z","categories_index":"算法","tags_index":"算法","author_index":"碔砆"},{"id":"a0a717998dbf88c81be122d0bce3bca3","title":"信息论","content":"Information Theory\r\nMLAPP 2.8\r\nPreface\r\n信息论(Information\r\nTheory)：应用数学的一个分支，涉及用紧凑的方法来表示数据（如数据压缩和编码），以及具有鲁棒性的储存和传输数据。在机器学习中，信息论常常应用于连续型变量\r\n信息论的基本想法是：一个不太可能发生的事发生了，要比一个非常可能的事件发生，提供更多的信息。尤其是：\r\n\r\n非常可能发生的事件信息量要比较少，且极端条件下，必然事件应该没有信息量\r\n较不可能发生的事件具有更高的信息量\r\n独立事件应具有增量的信息\r\n如：英语中的常见词 a the等要比稀有词短的多\r\n\r\n因此，我们需要一个模型来预测哪种数据是可能的，哪些是不可能的，这就是机器学习领域所关注的问题\r\n为满足上述提到的3个性质，定义一个事件x的自信息(self-information)\r\n为\r\n\r\n定义的单位是奈特，一奈特是以的概率观测到一个事件时获得的信息量，还有其他以2为底数的对数，它们的单位是比特或者香农\r\n熵\r\n自信息只能处理单个的输出，对整个概率分布中的不确定性总量进行量化，我们使用香农熵(Shannon\r\nentropy)\r\n具体的说，对具有K个状态的离散量，其定义如下：\r\n从定义中可以看出：一个分布的香农熵是指遵循这个分布的事件所产生的期望信息总量\r\n它给出了对依据概率分布P生成的符号进行编码所需的比特数在平均意义是的下界\r\n那些接近确定性的分布具有较低的熵，那些接近均匀分布的概率分布具有较高的熵\r\n当x是连续时，香农熵被称为微分熵(differential\r\nentropy)\r\nKL散度\r\n当对于一个随机变量x有两个单独的概率分布，我们使用KL散度(Kullback-Leibler(KL)\r\ndivergence) 来衡量这两个分布的差异\r\n\r\n其中是交叉熵(cross-entropy)，其定义为\r\n\r\n在离散型变量的情况下，KL散度衡量的是，当我们使用一种被设计成能够使得概率分布Q产生的信息的长度最小的编码，发送包含由概率分布P产生的符号的信息时，所需要的额外信息量\r\n交叉熵指，当我们使用模型q时，使用分布为p的数据进行编码所需要的平均比特数来定义编码\r\nKL散度是非负的，当且仅当P和Q在离散型变量的情况下是相同的分布，或者在连续型变量的情况下是”几乎处处“相同的\r\n因为KL散度是非负的并且衡量的是两个分布之间的差异，常被用作分布之间的某种举例，但KL并不是真的距离，它并不对称\r\n互信息\r\n对于两个随机变量X，Y，想知道一个变量能告诉我们多少关于另一个变量的信息。对实值随机变量，我们可以计算这两个随机变量的相关系数。但更一般的方法是确定联合分布与因式分解分布之间的相似性，这就引出了互信息(mutual\r\ninfromation) 的概念，其定义如下：\r\n\r\n当且仅当时，,即当且仅当变量是独立的时候，\r\n参照KL散度的定义，互信息也可写为以下形式：\r\n\r\n其中是条件熵(conditional\r\nentropy)，定义为\r\n\r\n从上式可看出，互信息可解释为在观察到Y之后，X的不确定的减小\r\n与互信息关系密切的一个概念是逐点互信息(pointwise mutual\r\ninformation)，其定义为：\r\n\r\n其衡量了这些事件 一起发生时与各自偶然发生时 之间的差异\r\n连续型随机变量的互信息\r\n","slug":"信息论","date":"2023-02-18T10:32:18.000Z","categories_index":"AI学习","tags_index":"AI学习","author_index":"碔砆"},{"id":"665540b83d48c62e220c6e1d58a9965e","title":"卷积神经网络","content":"卷积神经网络\r\n卷积网络(convolutional network)\r\n又称卷积神经网络(CNN)，是专门用来处理具有类似网格结构的数据的神经网络\r\n通常指那些至少在网络的一层中使用卷积运算来代替一般的矩阵乘法运算的神经网络\r\n卷积运算\r\n卷积是对两个实变函数的一种数学运算\r\n在许多实际问题中，经常需要对所求值沿特定轴进行加权平均，如时间上越近的测量结果越相关\r\n这可以通过一个加权函数实现，表示测量结果距当前时刻的时间间隔，表示是与当前时刻相关的函数\r\n对任意时刻都采取这种加权平均计算，就可以得到一个新的平滑估计函数s：\r\n\r\n称这种运算为卷积(convolution)，通常用星号表示\r\n\r\n在卷积网络的术语中，卷积的第一个参数通常叫做输入(input)\r\n第二个参数叫做核函数(kernel\r\nfunction)，输出有时被称为特征映射(feature\r\nmap)\r\n在机器学习中，输入经常是多维数组的数据，称为张量，核函数通常是由学习算法优化得到的参数\r\n二维的核写作以下形式，卷积是可交换的\r\n\r\n卷积的可交换性的出现是因为我们将核相对输入进行了翻转(flip)\r\n从m增大的角度来看，输入的索引在增大，但是核在减小\r\n许多库会实现另外一个相关函数，称为互相关函数，和卷积运算基本一样，但是没有对核进行翻转\r\n\r\n实际运算步骤为，依据核的大小，在张量上逐行逐列的移动来得到输入\r\n离散卷积可以看作矩阵的乘法，但这个矩阵的元素被限制为必须核另外一些元素相等\r\n如在单变量的离散卷积中，矩阵每一行中的元素都与上一行对应位置平移一个单位的元素相等\r\n这种矩阵叫做Toeplitz矩阵(Toeplitz matrix)\r\n对于二维情况，卷积对应着一个双重分块循环矩阵\r\n除此之外，卷积通常对应着一个非常稀疏的矩阵，这是因为核的大小要远远小于张量的大小\r\n任何一个使用矩阵乘法且不依赖于矩阵结构的特殊性质的神经网络算法都适用于卷积运算\r\ndef corr2d(X, K):\n# 卷积计算\n\th, w = K.shape\n\tY = torch.zeros((X.shape[0] - h + 1), (X.shape[1] - w + 1))\n\tfor i in range(Y.shape[0]):\n\t\tfor j in range(Y.shape[1]):\n\t\t\tY[i, j] = (X[i:i + h, j:j + w] * K).sum()\n\treturn Y\n\nclass Conv2D(nn.module):\n\t\r\npytorch——Conv2d\r\n张闯Pytorch\r\n选择卷积的动机\r\n之前我们已经提过一种名为全连接层的前向传播的神经网络结构，那么又为何要改为选择卷积呢？\r\n主要有以下三个重要的思想：\r\n稀疏交互(sparse interactions)\r\n传统的神经网络使用的是矩阵乘法，这使得每一个输出单元与每一个输入单元都会产生交互\r\n相比之下，卷积网络具有稀疏交互的特征\r\n由于核的大小远小于输入的大小，因此我们可以通过较小的核从较大的图像中提取出一些小的且有意义的特征\r\n这使得需要存储的参数更少，不仅减少了模型的存储需求，还提高了它的统计效率\r\n而在大多数分类问题中，我们不需要关注相隔很远的元素之间的联系，只需要关注邻近的几个元素就足以分类\r\n参数共享(parameter sharing)\r\n参数共享是指在一个模型的多个函数中使用相同的参数\r\n传统的神经网络中，计算一层的输出时，权重矩阵的一个元素只使用一次\r\n而在参数共享下，用于一个输入的权重也会被绑定在其他的权重上，也被称为每一个网络具有绑定的权重\r\n在CNN在，核的每一个元素都作用在输入的每一位置上\r\n这保障了我们在训练阶段也只需要线学习一个参数集合，而无需对每一个位置都学习专门的参数\r\n这使得卷积在存储需求核统计效率方面都有很大的提升\r\n等变表示(equivalen\r\nrepresentations)\r\n首先，先说明分类问题的一个很基础的性质：平移不变性(translation\r\ninvariance)\r\n即不管检测对象出现在图像的哪个位置，神经网络的前面几层对相同的图像应该有相似的反应\r\n而在卷积中，由于参数共享的特殊形式，使得神经网络具有对平移等变的性质\r\n如果一个函数满足输入改变，输出也以同样的方式改变的这一性质，就称为等变\r\n这意味着，检测对象在输入中的改变，仅导致隐藏表示的变化，即\r\n\r\n其中，是权重的重新索引，\r\n又因为要使与，无关，故使得\r\n池化\r\n卷积网络中的一个典型层包括三级：卷积级、探测级、池化级\r\n在卷积级中，这一层并行地计算多个卷积产生一组线性激活响应\r\n在探测级中，每一个线性激活响应将会通过一个非线性的激活函数\r\n在池化级中，我们通过池化函数来进一步调整这一层的输出\r\n池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出\r\n\r\n不管使用怎样的池化函数，当输入做少量平移时，池化能帮助输入的表示近似不变\r\n局部不变性在我们只关心某个特征而不关心其出现的位置时非常有用\r\n使用池化可以看作增加了一个无限强的先验：这一层学得的函数必须具有对少量平移的不变性\r\n因为池化综合了所有邻居的反馈，这使得池化单元少于检测单元成为可能\r\n这种方法可以减少下一层约k倍的输入，从而提高了网络的计算效率\r\n同时，池化对于处理不同大小的输入具有重要作用\r\n如当我们需要对不同大小的图像进行分类时，分类成的输入必须是固定的大小，而这常通过调整池化区域的偏置大小来实现\r\n但与此同时，池化可能会使得一些利用自顶向下信息的神经网络结构变得复杂，如玻尔兹曼机和自编码器\r\n最大池化层和平均池化层\r\n与卷积层类似，汇聚层运算符由一个固定形状的窗口组成，该窗口根据其步幅大小在输入的所有区域上滑动，为固定形状窗口（有时称为汇聚窗口）遍历的每个位置计算一个输出\r\n然而，不同于卷积层中的输入与卷积核之间的互相关计算，汇聚层不包含参数\r\n相反，池运算是确定性的，我们通常计算汇聚窗口中所有元素的最大值或平均值\r\n这些操作分别称为最大汇聚层（maximum\r\npooling）和平均汇聚层（average pooling）\r\n多通道\r\n在处理多通道输入数据时，汇聚层在每个输入通道上单独运算，而不是像卷积层一样在通道上对输入进行汇总\r\n这意味着汇聚层的输出通道数与输入通道数相同\r\n卷积和池化作为一种无限强的先验\r\n先验被认为强或者弱取决于先验中概率密度的集中程度\r\n弱先验具有较高的熵值，这允许数据对于参数的改变具有或多或少的自由性\r\n强先验具有较低的熵值，它在决定参数最终取值时起到更为积极的作用\r\n一个无限强的先验需要对一些参数的概率置零并且完全禁止对这些参数赋值，无论数据对这些参数起到多大的支持\r\n把卷积网络类比成全连接网络，就会对这个全连接网络的权重增加一个无限强的先验\r\n这个先验表示一个隐藏单元的权重必须和它的邻居权重相同，但可以在空间上移动\r\n这个先验也要求除了那些处在隐藏单元的小的空间连续的接受域内的权重外，其他的权重都为零\r\n这个先验说明了该层应该学得的函数只包含局部连接关系并且对平移具有等变性\r\n而从这种类比中，我们可以看出：卷积和池化都可能导致欠拟合\r\n另一个关键是当我们比较卷积模型的统计学习表现时，只能以基准中的其他卷积模型作为比较的对象\r\n基本卷积函数的变体\r\n填充\r\n假设输入形状为，卷积核形状为，那么输出形状将是\r\n因此，卷积的输出形状取决于输入形状和卷积核的形状\r\n而有效的调整输出形状的方式就是 填充(padding)\r\n，以及之后介绍的 步幅(stride)\r\n如上所述，我们在应用多层卷积时，很容易丢失边缘像素，常见的解决方法就是在边界处进行填充\r\n如果我们要填充行列，常见的填充方法是在上下左右各填充1/2\r\n\r\n完成填充操作后，输出形状将变为\r\n以下将介绍三种特别的零填充设置：\r\n\r\n有效卷积：无论如何都不使用零填充的极端情况，卷积核只允许访问图像中那些能够包含整个核的位置\r\n相同卷积：只进行足够的零填充来保持输入和输出具有相同的大小，这种情况会导致输入像素中靠近边界的部分相对于中间部分对于输出像素的影响更小，会导致边界像素存在一定程度的欠表示\r\n全卷积：进行了足够多的零填充，使得每个函数在每个方向上恰好被访问了k次，这种情况下，输出像素靠近边界的部分相比于中间部分是更少像素的函数，导致很难找到一个在卷积映射所有位置表现良好的单核\r\n\r\n通常来讲，零填充的最佳数量在于有效卷积和相同卷积中的某个位置\r\n步幅\r\n在计算互相关时，卷积窗口从输入张量的左上角开始，向下向右滑动，默认每次滑动一个元素\r\n但是，有时候为了高效计算或是缩减采样次数，卷积窗口可以跳过中间位置，每次滑动多个元素\r\n而间隔的像素数就被称为步幅，我们可以对不同方向设置不同的步幅\r\n增加每个方向上相同步幅s后的采样卷积函数变为：\r\n\r\n通常，当垂直步幅为、水平步幅为时，输出形状为\r\n卷积层中的多输入多输出通道\r\n在大多数图像问题中，一张图片由RGB三个通道构成，而之前的讨论中都聚焦于二维张量\r\n因此，我们有必要去在二维张量中增添通道，使之成为三维张量\r\n多输入通道\r\n而当输入具有多个通道时，需要构造一个与输入具有相同通道数的卷积核\r\n当通道数时，我们卷积核的每个输入通道将包含形状为的张量\r\n将这些张量连结在一起可以得到形状为的卷积核\r\n由于输入和卷积核都有个通道，我们可以对每个通道输入的二维张量和卷积核的二维张量进行互相关运算，再对通道求和（将的结果相加）得到二维张量\r\n\r\n这是多通道输入和多输入通道卷积核之间进行二维互相关运算的结果\r\n多输出通道\r\n随着神经网络层数的加深，我们常会增加输出通道的维数，通过减少空间分辨率以获得更大的通道深度\r\n直观地说，我们可以将每个通道看作对不同特征的响应\r\n而现实可能更为复杂一些，因为每个通道不是独立学习的，而是为了共同使用而优化的\r\n因此，多输出通道并不仅是学习多个单通道的检测器\r\n用和分别表示输入和输出通道的数目，为了获得多个通道的输出，我们可以为每个输出通道创建一个形状为\r\n的卷积核张量，这样卷积核的形状是\r\n在互相关运算中，每个输出通道先获取所有输入通道，再以对应该输出通道的卷积核计算出结果\r\n局部连接层\r\n平铺卷积\r\n1\r\n1卷积\r\n","slug":"卷积神经网络","date":"2023-02-18T10:24:15.000Z","categories_index":"AI学习","tags_index":"AI学习","author_index":"碔砆"},{"id":"60fb8c126ebe52ae9e85a9be67ca6bb2","title":"贝叶斯统计","content":"贝叶斯统计\r\nMLAPP 149\r\nDL 85\r\n5.1 Introduction\r\n区别于频率派统计方法和基于估计单一值的方法：基于该估计做所有的预测\r\n另一种方法是在做预测时就考虑所有可能的，后者属于贝叶斯统计(Bayesian\r\nstatistics)\r\n贝叶斯统计的核心在于：使用后验分布来总结我们所知道的关于一组未知变量的一切\r\n频率派的视角是真实参数是未知的定值，而点估计是考虑数据集上函数的随机变量\r\n而贝叶斯统计用概率来反映知识状态的确定性程度，数据集是可观测的，而真实参数是未知或不确定的，因此表示为随机变量\r\n在观察到数据前，我们将的已知知识表示为先验概率分布(prior\r\nprobability distribution) 简称为先验\r\n当我们观测到m个样本后，对第m+1个样本的预测如下：\r\n\r\n可以看出，每个具有正概率密度的都有助于下一个样本的判断，其中贡献有后验密度本身加权\r\n而在观测完数据集D后，若我们仍然不确定的值，那么这个不确定性会直接包含在之后的预测中\r\n当训练数据小时，贝叶斯方法通常泛化的很好，但在训练数据大时，所产生的计算成本很大\r\n5.2 后验分布的总结\r\n5.2.1 MAP估计\r\n通过计算后验均值、中值(median)和模态(mode)我们可以很轻易的得到一个未知量的点估计\r\n通常情况下，后验均值或中值是实值量的最佳选择，后验边缘向量是离散量的最佳选择\r\n然而最受欢迎，也最为常用的后验模型，也就是 最大后验(Maximum A\r\nPosteriori，MAP) 估计\r\nMAP估计将问题简化为了一个优化问题，通过选择后验概率最大的点或是在是连续型的更常见情况下，概率密度最大的点:\r\n\r\n右边的对应着标准的对数似然项，对应着先验分布\r\n尽管MAP估计非常好用，但其并不完善，接下来将介绍一些它的缺点\r\n5.2.1.1 没有不确定度的度量\r\nMAP估计及其他任何点估计都具有一个缺点：缺少一个对不确定度的度量值，这在许多问题中是非常重要的\r\n5.2.1.2\r\n插入MAP估计容易导致过拟合\r\n由于在机器学习中，我们更关注预测的准确性，而不是如何解释模型的参数\r\n但如果不去建模参数中的不确定性，则容易导致模型的预测分布过度自信，从而导致过拟合\r\n5.2.1.3 模态是一个非典型点\r\n选择模态作为后验分布通常是一个很糟糕的选择，因为模态对分布而言通常是非典型的，例子可见MLAPP150F5.1\r\n究其原因是，模态是一个衡量零值的点，而均值和中值考虑了空间的体积\r\n同时，在偏态分布(例子见MLAPP150F5.1)中，也常发生模式为0，但均值不为0的现象\r\n这种分布经常出现在推断方差参数时，尤其是在层次模型中，这使得MAP的表现非常糟糕\r\n当MAP为首的模式不好用时，我们常通过决策理论来总结后验分布\r\n5.2.2 可信区间\r\n5.2.1.1中提出，MAP估计缺少一个对不确定度的度量，而对一个标量参数的置信度的度量就是其后验分布的宽度\r\n这一宽度可以用的\r\n可信区间(credible interval) 来衡量，可以用区间来表示\r\n\r\n这样的区间也许有很多个，因此我们在每一尾中选择质量为的子区间，称其为中心区间(central\r\ninterval)\r\n如果后验分布有一个已知的函数表达式，我们可以通过其分布函数来计算可信区间\r\n\r\n而当我们不知道后验分布的形式时，我们可以从后验分布中选取样本，之后再使用蒙特卡罗近似计算后验分位数\r\n排序个样本，并沿着排序后的列表来找到第的样本，当时，这个样本就成为真正的分位数\r\nTips：可信区间(credible interval)和置信区间(confidence\r\ninterval)要区分开，前者属于贝叶斯统计，后者属于概率统计\r\n5.2.3 在比例差异中的推理\r\n在现实生活中的很常见的例子：商品1有90个好评和10个差评，商品2有2个好评0个差评，该如何选择\r\n对这一问题，我们常通过贝叶斯分析，这一方法也可推广到其他关于比例的情况下\r\n详细过程见MLAPP 155(TO DO)\r\n另一种方法就是通过蒙特卡罗抽样来近似得出二者的后验分布\r\n5.3 贝叶斯模型选择\r\n在一个机器学习问题中，超参数的选择不当常常会引发过拟合或者欠拟合的后果，由此便引出了模型选择这一问题\r\n模型选择的一种方法就是利用机器学习基础2.3节中介绍的交叉验证来估计所选模型的泛化误差\r\n而另一种更有效的方法就是计算所选模型的后验分布\r\n\r\n从中，我们可以很快的计算出MAP模型：，这就是贝叶斯模型选择\r\n如果我们使用一致的先验分布，，则相当于选择最大化的模型\r\n我们称这个量为\r\n边际似然(marginal likelihood)\r\n、微分似然(integrated likelihood) 或\r\n模型m的证据(evidence)\r\n","slug":"贝叶斯统计","date":"2023-02-18T10:23:28.000Z","categories_index":"AI学习","tags_index":"AI学习","author_index":"碔砆"},{"id":"e9525b7ef8e5ead70d9f4aba30aa5c7e","title":"机器学习基础","content":"机器学习基础\r\nDL\r\n1.学习算法\r\n机器学习算法是一种能够从数据中学习的算法，可定义为\r\n”对于某类任务T和性能度量P，一个计算机程序被认为可以从经验E中学习是指，通过经验E改进后，它在任务T上由性能度量P衡量的性能有所提升\"\r\n1.1 任务T\r\n学习的过程本身并不是任务，学习是所谓获取完成任务的能力的过程\r\n通常来讲，机器学习任务被定义为 机器学习系统应该如何处理\r\n样本(example)\r\n样本是指我们从某些希望机器学习系统处理的对象或事件中收集到的已经量化的\r\n特征(feature) 的集合\r\n1.2 性能度量P\r\n为评估机器学习算法的能力，设计其性能的定量度量是有必要的\r\n通常性能度量P是特定于任务T的，如分类型任务，性能度量P为准确率acc\r\n对性能度量P的衡量主要聚焦于未观测数据，即 测试集(test\r\nset) 上\r\n对于很多模型，选择一个与系统理想表现对应的性能度量通常是很有难度的\r\n1.3 经验E\r\n根据学习过程中的不同经验，机器学习算法可以大致分类为\r\n无监督算法 和 有监督算法\r\n二者间主要区别来自于这样的一个视角：老师提供目标y给机器学习系统，指导其应该作什么。在无监督算法中则没有老师这样的角色，算法必须在没有指导的情况下理解数据\r\n2. 容量、过拟合和欠拟合\r\n机器学习的重要目的就是在观测到的数据，即 训练集\r\n上的表现良好之外，也在测试集上表现良好\r\n这之中主要涉及一个算法的 泛化(generalization)\r\n能力\r\n统计学习理论对泛化能力的提升给出了相应的方法：\r\n训练集和测试集数据通过数据集上被称为 数据生成过程(data\r\ngenerating process) 的概率分布生成。而在这一过程中采取\r\n独立同分布(i.i.d assumption)\r\n的假设，对提升泛化能力非常重要\r\n独立同分布指，每个数据集中的样本都是彼此相互独立的，并且训练集和测试集是同分布的\r\n这么做可以使得随机模型的训练误差期望和测试误差期望是一致的\r\n从中，我们可以延申出决定机器学习算法效果是否好的两个因素：\r\n\r\n降低训练误差\r\n缩小训练误差和测试误差的差距\r\n\r\n而这两个因素又分别对应着机器学习的两个主要挑战：欠拟合(underfitting)\r\n和 过拟合(overfitting)\r\n欠拟合指模型不能在训练集上获得足够低的误差，过拟合指训练误差和测试误差之间的差距过大\r\n由此又另外延申出一个模型的概念：容量(capacity)\r\n容量指一个模型拟合各种函数的能力，容量过高则会使模型记住了不适于测试集的训练集性质从而表现为过拟合\r\n一种常见的控制容量的方法是选择 假设空间(hypothesis\r\nspace) ，即学习算法的选择范围为解决方法的函数集\r\n主要控制容量的方法是：改变输入特征的数目和加入这些特征对应的参数\r\n而在这一过程中，应当遵循 奥卡姆剃刀原理\r\n对于容量任意高的极端情况，则归于 非参数模型\r\n的概念\r\n非参数模型，指模型的参数会随着输入特征的变多而变多\r\n与参数模型相比，非参数模型更为灵活，所需要的假设更少，但一个严重的问题在于对于大部分数据而言计算困难\r\n2.1 没有免费午餐定理\r\n对于不同的问题，通常需要设计不同的模型，而学习理论也表明机器学习算法可以在有限个训练集样本中很好地泛化\r\n但是，必须强调，不存在万能的最佳模型，即 没有免费午餐定理(no\r\nfree lunch theorem)\r\n这一理论表明，在所有可能的数据生成分布上平均之后，每一个分类算法在未事先观测的点上具有相同的错误率\r\n但必须强调，这一定理仅在考虑所有可能的数据生成分布时才成立\r\n在实际应用中，我们常对所遇到的概率分布进行假设，并针对该假设设计表现良好的算法\r\n2.2 正则化(TO DO)\r\n正则化(regularization)\r\n是指修改学习算法，使其降低泛化误差而非训练误差\r\n2.3 超参数和验证集\r\n超参数(hyper-parameter)\r\n常被设置用以控制算法的行为，超参数不是通过学习算法本身学习出来的\r\n常见的设置超参数的原因是该参数不适合在训练集上学习，如控制模型容量的所有超参数\r\n如果在训练集上训练这些超参数，这些超参数总是会区域最大可能的模型容量从而导致过拟合\r\n为解决这个问题，引出一个训练算法观测不到的 验证集(validation\r\nset) 样本\r\n验证集重要用于挑选模型的超参数\r\n验证集的重点在于测试样本不能以任何形式参与到模型的选择之中，包括设置超参数\r\n也是因此，之前提到的测试集中的样本不能用于验证集\r\n通常情况下，经常将用于学习参数的训练集中挑选子集来构建验证集，比例常见为4：1\r\n2.3.1 交叉验证\r\n在训练中，若训练集的误差很小，在数据集太小时，可能会带来一些问题\r\n一个小规模的测试集意味着平均测试误差估计的统计不确定性，使得很难判断算法A、B之间在给定任务上的优劣\r\n常见的一个解决方法为k-fold交叉验证\r\n算法如下：将训练集分为k个互斥子集，训练除第k个子集外的每一个子集，并在第k个子集上进行测试，以这种方式进行滚动式循环训练。之后，计算所有子集的平均误差，并以此来代替测试误差\r\n（tips：每个点只用于一次测试，和n-1次训练）\r\n2.4 估计、偏差和方差（TO DO）\r\n2.4.1 点估计\r\n2.4.2 偏差\r\n2.4.3 方差和标准差\r\n2.4.4\r\n权衡偏差和方差以最小化均分误差\r\n2.4.5 一致性\r\n2.5 最大似然估计\r\nMLAPP 217 估计统计模型参数的一种常见方法是计算其最大似然估计(Maximum\r\nlikelihood estimation，MLE) 对的最大似然估计定义为：\r\n\r\n为简便运算，常将原函数进行取对数处理\r\n在实际进行代码运行时，我们更经常等价地采取最小化负对数的似然\r\n一种解释最大似然估计的观点是将它看作最小化训练集上的经验分布和数据之间的差异，二者间的差异程度由KL散度度量\r\n\r\n其中左边一项仅涉及数据生成过程，因此只需要最小化\r\n2.5.1 条件对数似然和均方误差（to\r\ndo）\r\n由之前最大似然估计的定义，可进一步拓展到估计条件概率从而给定x预测y\r\n而这就构成了大多数监督学习的基础\r\n2.5.2 最大似然的性质\r\n最大似然估计的最重要的一点在于，它被证明了当样本数目时，就收敛率而言是最好的渐进估计\r\n在以下条件下，最大似然估计具有一致性\r\n\r\n真实分布必须在模型族中，否则，没有估计可以还原\r\n真实分布必须刚好对应一个值，否则，最大似然估计恢复除真实分布后，也不能决定数据生成过程使用哪个\r\n\r\n除了最大似然估计之外，还有其他的归纳准则，且都具有一致性\r\n但这些一致估计的 统计效率(statistic efficiency)\r\n可能区别很大\r\n在综合考虑了一致性和统计效率之后，最大似然通常是机器学习中的首选估计方法\r\n2.6 贝叶斯统计\r\n2.7 监督学习算法\r\n2.8 无监督学习算法\r\n2.9 随机梯度下降\r\n","slug":"机器学习基础","date":"2023-02-18T08:33:33.000Z","categories_index":"AI学习","tags_index":"AI学习","author_index":"碔砆"},{"id":"563a71603f1af2b0a426978adee1756b","title":"二叉树","content":"二叉树\r\n树\r\n定义\r\n一棵树t是一个非空的有限元素的集合，其中一个元素为根（root），其余元素组成t的子树（subtree）\r\n在层次数据中最高层的元素是根，其直接下一级元素是子树的根\r\n树的画法\r\n在画一棵树时，每一个元素都代表一个节点。树根画在最上方，其子树画在下面。在根和子树的根之间有一条边。子树结构相同。\r\n在树中没有子元素的元素称为叶（leaf）\r\n术语\r\n级 ：树根是1级，子节点是父节点级树+1\r\n高度（深度） ：树中级的个数\r\n度：一个结点的子节点个数，一棵树的度是其元素的度的最大值\r\n二叉树\r\n定义\r\n一棵二叉树（binary tree）\r\nt是有限个元素的集合（可以为空）。当二叉树非空时，其中有一个元素称为根，余下的元素被划分为两颗二叉树，分别称为t的左子树和右子树。\r\n和树的根本区别\r\n\r\n二叉树的每个元素都恰好有两颗子树。而树的每个元素可有任意数量的子树\r\n在二叉树中，每个元素的子树都是有序的，有左子树和右子树之分。而树的子树是无序的\r\n\r\n二叉树的特性\r\n特性1 一棵二叉树有n个元素，n&gt;0，它有n-1条边\r\n特性2\r\n一棵二叉树的高度为h，h&gt;=0，它最少有h个元素，最多有2h\r\n-1个元素\r\n特性3\r\n一棵二叉树有n个元素，n&gt;0，它的高度最大为n，最小高度为[log2(n+1)]\r\n特性4\r\n一棵非空二叉树的第i层上至多有2i-1个节点\r\n满二叉树 当高度为h的二叉树恰好有2h\r\n-1个元素时，称其为满二叉树。\r\n完全二叉树\r\n对高度为h的满二叉树的元素，从第一层到最后一层，在每一次中从左至右，顺序编号，从1到2h\r\n-1。从中删除k个其编号为\r\n2h-i元素，1&lt;=i&lt;=k&lt;2h,所得到的二叉树被称为完全二叉树，即按从上到下，从左到右的顺序编号，号码与满二叉树一致\r\n特性5\r\n设完全二叉树的一元素其编号为i，1&lt;=i&lt;=n。有以下关系成立\r\n\r\n如果i=1，则该元素为二叉树的根。若i&gt;1，则其父节点的编号为 i/2\r\n如果2i&gt;n，则该元素无左孩子。否则，其左孩子的编号的编号为2i\r\n如果2i+1&gt;n，则该元素无右孩子。否证，其右孩子的编号为 2i+1\r\n\r\n二叉树的描述\r\n数组描述：按照编号存储在数组的相应位置里。\r\n仅在缺少的元素数目比较少时适用\r\n链表描述：\r\ntemplate &lt;class T>\nstruct binaryTreeNode\n&#123;\n    T element;\n    binaryTreeNode&lt;T> *leftChild,*rightChild;\n    binaryTreeNode() &#123;leftChild = rightChild = NULL;&#125;\n    binaryTreeNode(const T&amp; theElement,\n                   binaryTreeNode *theLeftChild = NULL,\n                   binaryTreeNode *theRightChild = NULL)\n    &#123;\n        element(theElement)\n        leftChild = theLeftChild;\n        rightChild = theRightChild;\n\t&#125;\n&#125;\r\n二叉树的操作\r\n前序遍历\r\ntemplate &lt;class T>\nvoid preOrder(binaryTreeNode&lt;T> *t)\n&#123;\n    if(t != NULL)\n    &#123;\n        cout&lt;&lt;t->element;\n        preOrder(t->leftChild);\n        preOrder(t->rightChild);\n\t&#125;\n&#125;\r\n中序遍历\r\ntemplate &lt;class T>\nvoid inOrder(binaryTreeNode&lt;T> *t)\n&#123;\n    if(t != NULL)\n    &#123;\n        inOrder(t->leftChild);\n        cout&lt;&lt;t->element;\n        inOrder(t->rightChild);\n\t&#125;\n&#125;\r\n后序遍历\r\ntemplate &lt;class T>\nvoid postOrder(binaryTreeNode&lt;T> *t)\n&#123;\n    if(t != NULL)\n    &#123;\n        postOrder(t->leftChild);\n        postOrder(t->rightChild);\n        cout&lt;&lt;t->element;\n\t&#125;\n&#125;\r\n层序遍历\r\ntemplate &lt;class T>\nvoid levelOrder(binaryTreeNode&lt;T> *t)\n&#123;\n    queue&lt;binaryTreeNode&lt;T>*> q;\n    while(t != NULL)\n    &#123;\n        q.push(t);\n        if(t->leftChild != NULL)\n            q.push(t->leftChild);\n        if(t->rightChild != NULL)\n            q.push(t->rightChild);\n        try &#123;t = q.front();&#125;\n        catch (queueEmpty) &#123;return;&#125;\n        q.pop();\n    &#125;\n&#125;\r\n","slug":"二叉树","date":"2022-04-20T01:49:36.000Z","categories_index":"数据结构","tags_index":"数据结构","author_index":"碔砆"},{"id":"b96df039eaa03d2a5634c2cc83e5cad0","title":"线性表","content":"\r\n","slug":"线性表","date":"2022-04-20T01:41:09.000Z","categories_index":"","tags_index":"","author_index":"碔砆"},{"id":"9fc7aaa29955e8777b63cdc5e2f3c111","title":"快速幂","content":"快速幂\r\n快速幂——OIWiki\r\n在O(logn)的时间内计算 an\r\n可以运用于模意义下取幂、矩阵幂等运算\r\n算法描述\r\n二进制取幂的想法是，我们将取幂的任务按照指数的 二进制表示\r\n来分割成更小的任务。\r\n因为有个二进制位，因此当我们知道了后，我们只用计算O(logn)次乘法就可以计算出\r\n中任意一个元素都是前一个元素的平方\r\n算法实现\r\nlong long binpow(long long a, long long b) {\n  if (b == 0) return 1;\n  long long res = binpow(a, b / 2);\n  if (b % 2)\n    return res * res * a;\n  else\n    return res * res;\n}\r\n矩阵快速幂\r\n基本公式：\r\n由的递推公式决定\r\n实现方法\r\nstruct Matrix\n{\n    long long m[15][15]={0};\n    Matrix operator* (const Matrix &amp;b) const\n    {\n        Matrix res;\n        for (int i = 1; i &lt;= n; i++)\n            for (int j = 1; j &lt;= n; j++)\n                for (int k = 1; k &lt;= n; k++)\n                    res.m[i][j] = (res.m[i][j] + (m[i][k] * b.m[k][j])%mod) % mod;\n    return res;\n    }\n};\nMatrix qpow(Matrix base,int b){      //ans应初始化为单位矩阵\n    Matrix ans;\n    for(int i=1;i&lt;=n;i++)\n        ans.m[i][i]=1;\n    while (b) {\n        if (b &amp; 1) ans = ans * base;\n        base = base * base;\n        b &gt;&gt;= 1;\n    }\n    return ans;\n}\r\n最后的答案根据题目另外写\r\n","slug":"快速幂","date":"2022-02-14T03:22:15.000Z","categories_index":"算法","tags_index":"算法","author_index":"碔砆"},{"id":"4aa58050e03a812b4fa7990ab0778c08","title":"二分匹配","content":"二分匹配\r\nbool dfs(int x)\n&#123;\n    for(int i=1;i&lt;=n;i++)\n    &#123;\n        if(mapp[x][i]&amp;&amp;(!used[i]))\n        &#123;\n            used[i]=1;\n            if(linker[i]==-1||dfs(linker[i]))\n            &#123;\n                linker[i]=x;\n                return true;\n            &#125;\n        &#125;\n    &#125;\n    return false;\n&#125;\nint hungary()\n&#123;\n    int ans=0;\n    memset(linker,-1,sizeof(linker));\n    for(int i=1;i&lt;=n;i++)\n    &#123;\n        memset(used,0,sizeof(used));\n        if(dfs(i)) ans++;\n    &#125;\n    return ans;\n&#125;\r\n","slug":"二分匹配","date":"2022-01-17T08:44:06.000Z","categories_index":"算法","tags_index":"算法","author_index":"碔砆"},{"id":"8fd51d91ee8f082030932013cc236c76","title":"图的存储","content":"图的存储\r\n直接存边\r\n使用一个数组来存边，数组中的每个元素都包含一条边的起点与终点\r\nstruct edge&#123;\n    int n,w;\n&#125;\nvector&lt;edge> e;\nvector&lt;bool> vis\nvoid dfs(int u) &#123;\n    if (vis[u]) return;\n    vis[u] = true;\n    for (int i = 1; i &lt;= m; ++i) \n        if (e[i].u == u)\n            dfs(e[i].v);\n&#125;\r\n遍历效率低下，一般仅用于Kruskal算法\r\n邻接矩阵\r\n使用一个二维数组adj来存边，其中adj[u][v]为1表示存在u到v的边，为0表示不存在\r\n如果是带边权的图，可以在 adj[u][v] 中存储\r\n一般只在稠密图中使用\r\n邻接表\r\n使用一个支持动态增加元素的数据结构构成的数组，如vector&lt;int&gt; adj[n + 1]来存边\r\n其中adj[u]存储的是点u的所有出边的相关信息（终点、边权等）\r\nvector&lt;bool> vis;\nvector&lt;vector&lt;int> > adj;\n\nbool find_edge(int u, int v) &#123;\n  for (int i = 0; i &lt; adj[u].size(); ++i)\n    if (adj[u][i] == v)\n      return true;\n  return false;\n&#125;\n\nvoid dfs(int u) &#123;\n  if (vis[u]) return;\n  vis[u] = true;\n  for (int i = 0; i &lt; adj[u].size(); ++i) \n    dfs(adj[u][i]);\n&#125;\n    adj[u].push_back(v);    //插入边\r\n存各种图都很合适，尤其适用于需要对一个点的所有出边进行排序的场合\r\n前向星\r\nstruct node&#123;\n\tint x,y,w,next;\n&#125;a[500005];\n\nint len,last[15000]; //len边的索引，last 点所连最后一条边的索引\n\nvoid ins(int x,int y,int w)&#123;\n\tlen++;\n\ta[len].x=x;a[len].y=y;a[len].w=w;\n\ta[len].next=last[x];last[x]=len;\n&#125;\n\nfor(int i=last[x];i;i=a[i].next) //遍历\r\n","slug":"图的存储","date":"2022-01-16T08:23:46.000Z","categories_index":"算法","tags_index":"算法","author_index":"碔砆"},{"id":"530e1936c4d6e837f2bf21a86f73c106","title":"最短路","content":"最短路\r\n性质\r\n对于边权为正的图，任意两个结点之间的最短路，不会经过重复的结点\r\n对于边权为正的图，任意两个结点之间的最短路，不会经过重复的边\r\n对于边权为正的图，任意两个结点之间的最短路，任意一条的结点数不会超过n，边数不会超过n-1\r\n记号\r\nn为图上点的数目，m为图上边的数目\r\ns为最短路的源点\r\nD(u)为s点到u点的实际最短路长度\r\ndis(u)为s点到u点的估计最短路长度\r\nw(u,v)为(u,v)这一条边的边权\r\nFloyd算法\r\n","slug":"最短路","date":"2022-01-16T07:42:55.000Z","categories_index":"算法","tags_index":"算法","author_index":"碔砆"},{"id":"48433a70b7e09e957d48f01fa1012852","title":"STL","content":"STL\r\nSTL——OIWiki\r\n共同点\r\n\r\n声明形式： 容器名&lt;数据类型&gt; 变量名\r\n迭代器：用来访问和检查STL容器中的元素的对象，与数据指针类似。主要支持自增（++）和解引用（*）运算符，其中自增用来移动迭代器，解引用可以获取或修改它指向的元素。\r\n用法：容器名&lt;数据类型&gt;::iterator（可用auto代替）\r\n共有函数\r\n= 赋值运算符以及复制构造函数\r\nbegin() 返回指向开头元素的迭代器\r\nend()\r\n返回指向末尾的下一个元素的迭代器。\r\nsize() 返回容器的元素个数\r\nmax_size() 返回容器理论上能储存最大的元素个数\r\nempty() 返回元素是否为空\r\nswap() 交换两个容器\r\nclear() 清空容器\r\n比较运算符 按字典序比较两个元素的大小\r\n\r\n序列式容器\r\nvector\r\nstd::vector 是STL提供的内存连续的，可变长度的数组。\r\n能提供线性复杂度的插入和删除，以及常数复杂度的随机访问\r\n优点\r\n\r\n可以动态分配内存\r\n重写了比较运算符和赋值运算符\r\n初始化简单，可以&#123; &#125;或者=\r\n\r\n初始化操作\r\nvector&lt;数据类型&gt; 变量名(初始空间，初始值)\r\n创建拷贝\r\nvector&lt;数据类型&gt; 变量名(被拷贝的变量名)\r\n移动整个容器\r\nvector&lt;数据类型&gt; 变量名(std::move(原容器))\r\n成员函数\r\nat(pos),operator[pos] 返回下标为pos的元素\r\nfront() 返回首元素的引用\r\nback() 返回末尾元素的引用\r\ndata() 返回数组第一个元素的指针\r\nresize() 改变vector的长度，多退少补\r\nreserve()\r\n使得vector预留一定内存空间，避免不必要的内存拷贝\r\ninsert() 支持在某个迭代器位置插入元素，线性复杂度\r\nerase()\r\n删除某个迭代其或者区间的元素，返回最后被删除的迭代器\r\npush_back() 在末尾插入一个元素\r\npop_back() 删除末尾元素\r\n### array\r\nstd::array是STL提供的内存连续、长度固定的数组数据结构,本质是对原生数组的直接封装\r\n#### 成员函数\r\nat(pos),operator[pos] 返回下标为pos的元素\r\nfront() 返回首元素的引用\r\nback() 返回末尾元素的引用\r\ndata() 返回数组第一个元素的指针\r\nfill(指定值) 以指定值填充容器\r\ndeque\r\nstd::deque是STL提供的双端队列数据结构\r\n成员函数\r\n同vector\r\npush_front() 在头部插入一个元素\r\npop_front() 删除头部元素\r\nlist\r\nstd::list 是STL提供的一个双向链表数据结构\r\n成员函数\r\n同deque\r\n关联式容器\r\nset\r\nset是关联容器，含有键值类型对象的已排序集\r\n内部通常采用红黑树实现\r\n插入与删除操作\r\ninsert(x) 当容器中没有等价元素的时候，将元素 x 插入到\r\nset 中。\r\nerase(x) 删除值为 x 的 所有\r\n元素，返回删除元素的个数。\r\nerase(pos) 删除迭代器为 pos\r\n的元素，要求迭代器必须合法。\r\nerase(first,last) 删除迭代器在\\([First,last)\\)范围内的所有元素。\r\n迭代器\r\nbegin() 返回指向首元素的迭代器，其中\r\n*begin = front\r\nend()\r\n返回指向数组尾端占位符的迭代器，注意是没有元素的。\r\nrbegin()\r\n返回指向逆向数组的首元素的逆向迭代器，可以理解为正向容器的末元素。\r\nrend()\r\n返回指向逆向数组末元素后一位置的迭代器，对应容器首的前一个位置，没有元素。\r\n查找操作\r\ncount(x) 返回键值为x的元素数量\r\nfind(x) 存在键值为x的元素时会返回该元素的迭代器\r\nlower_bound(x)\r\n返回指向首个不小于给定键的元素的迭代器。如果不存在这样的元素，返回\r\nend()\r\nupper_bound(x)\r\n返回指向首个大于给定键的元素的迭代器。如果不存在这样的元素，返回end()\r\nmap\r\nmap是有序键值对容器，它的元素的键是唯一的。map通常实现为红黑树\r\n搜索、移除和插入操作拥有对数复杂度\r\nmap&lt;Key,T&gt; mp #### 操作\r\n查询操作同set\r\n操作与删除操作同set\r\n可以直接通过下标访问来进行查询或插入操作。例如\r\nmp[\"Alan\"]=100\r\n通过向 map\r\n中插入一个类型为pair&lt;Key, T&gt;的值可以达到插入元素的目的，例如mp.insert(pair&lt;string,int&gt;(\"Alan\",100))\r\n容器适配器\r\n栈\r\nstd::stack是一种FILO的容器适配器\r\n操作\r\ntop() 访问栈顶元素\r\npop() 删除栈顶元素\r\npush(x) 向栈顶插入x元素\r\nempty() 询问容器是否为空\r\nsize() 查询容器中的元素数量\r\n队列\r\nstd::queue是一种FIFO的容器适配器\r\n操作\r\n同stack\r\nfront() 访问队首元素\r\n优先队列\r\nstd::priority_queue\r\n操作\r\n同queue\r\n其他非STL容器\r\nbitset\r\nstring\r\npair\r\nstd::pair\r\n是标准库中定义的一个类模板。用于将两个变量关联在一起，组成一个“对”，而且两个变量的数据类型可以是不同的。\r\n操作\r\n初始化\r\n可以在定义时直接完成pair的初始化\r\n也可以使用先定义，后赋值的方法完成pair的初始化\r\n还可以使用std::make_pair函数。\r\n该函数接受两个变量，并返回由这两个变量组成的pair\r\n一种常用的方法是使用宏定义#define mp make_pair，将有些冗长的make_pair化简为mp\r\nmake_pair可以配合auto使用，以避免显式声明数据类型\r\n访问\r\n通过成员函数first与second,可以访问pair中包含的两个变量\r\n比较\r\n&lt;、&gt;、&lt;=、&gt;=\r\n四个运算符会先比较两个pair中的第一个变量，在第一个变量相等的情况下再比较第二个变量\r\n算法部分\r\nnext_permutation\r\n通常用于生成序列的全排列\r\n","slug":"STL","date":"2022-01-15T12:53:20.000Z","categories_index":"算法","tags_index":"算法","author_index":"碔砆"},{"id":"4421686b118d9483ed6838244ba67f35","title":"背包DP","content":"背包DP\r\n背包DP——OIWiki\r\n0-1背包\r\n已知条件有第个物品的重量，价值，以及背包的总容量,每个物体只有取和不取两种状态\r\n设 DP 状态为在只能放前个物品的情况下，容量为的背包所能达到的最大总价值\r\n状态转移方程\r\n\r\n枚举时，从枚举到，保证总是在前被更新\r\nfor (int i = 1; i &lt;= n; i++)\n    for (int l = W; l &gt;= w[i]; l--)  \n        f[l]=max(f[l], f[l - w[i]] + v[i]);\r\n多种价值时开多维数组即可\r\n枚举时也多开一层循环 ## 完全背包 完全背包模型与 0-1 背包类似，与 0-1\r\n背包的区别仅在于一个物品可以选取无限次，而非仅能选取一次\r\n状态转移方程\r\n\r\n枚举时，从枚举到\r\n\r\ntips: 注意f[0]的初始化，保证f[0]=0\r\n\r\n多重背包\r\n多重背包也是 0-1 背包的一个变式。与 0-1 背包的区别在于每种物品有个，而非一个\r\n朴素想法：将每种物品的个，视为不同种物品\r\n二进制分组优化\r\n用代表第i种物品拆分出的第j个物品\r\n朴素法中存在问题:重复考虑了「同时选」与「同时选」这两个完全等效的情况\r\n通过二进制分组的方式使拆分方式更加优美\r\n具体地说就是令分别表示由个单个物品「捆绑」而成的大物品。特殊地，若不是2的整数次幂，则需要在最后添加一个由个单个物品「捆绑」而成的大物品用于补足\r\nindex = 0;\nfor (int i = 1; i &lt;= m; i++) {\n    int c = 1, p, h, k;     //c表示2进制\n    cin &gt;&gt; p &gt;&gt; h &gt;&gt; k;     //p价值，h体积，k数量\n    while (k - c &gt; 0) {\n        k -= c;\n        list[++index].w = c * p;\n        list[index].v = c * h;\n        c *= 2;\n    }\n  list[++index].w = p * k;  //补足\n  list[index].v = h * k;\n}\r\n混合背包\r\n将以上3种背包结合，分开求解即可\r\n\r\n","slug":"背包DP","date":"2022-01-15T07:21:57.000Z","categories_index":"算法","tags_index":"算法","author_index":"碔砆"},{"id":"f6af5bf3fb5f66f52237908153c03aea","title":"动态规划","content":"动态规划DP\r\n动态规划——OIWiki\r\n思路\r\n应用于子问题重叠的情况，即不同的子问题拥有公共的子问题\r\n（子问题的求解是递归进行的，将其划分为更小的子子问题）\r\n对于每个子子问题只求解一次，并将其保存在一个表格中。 ####\r\n最优子结构性质\r\n问题的最优解由相关子问题的最优解组合而成，而这些子问题可以独立求解 ###\r\n基础步骤 - 刻画最优解的结构特征（思考最优解的形式） -\r\n尝试递归的定义最优解的值（即考虑从\\(i-1\\)转移到\\(i\\)） - 计算最优解 -\r\n利用计算出的信息构造最优解\r\n实现方法\r\n\r\n带备忘的自顶向下法\r\n按自然递归形式编写，在过程中保存各个子问题的解\r\n自底向上法 将子问题按规模由从小到大的顺序求解\r\n\r\n","slug":"动态规划","date":"2022-01-15T01:55:19.000Z","categories_index":"算法","tags_index":"算法","author_index":"碔砆"},{"id":"c18469ab20e7b093d36b4a61728d07eb","title":"并查集","content":"并查集\r\n并查集——OIwiki\r\n并查集——知乎\r\n种类并查集——知乎\r\n应用\r\n\r\n并查集判环\r\n\r\n","slug":"并查集","date":"2022-01-13T07:25:39.000Z","categories_index":"算法","tags_index":"算法","author_index":"碔砆"},{"id":"827701aa0aeba949971d70c354c78372","title":"贪心算法","content":"贪心\r\n基本思路\r\n每一步行动总是采取最优解\r\n适用范围\r\n贪心算法在有最优子结构的问题中尤为有效。\r\n最优子结构的意思是问题能够分解成子问题来解决，子问题的最优解能递推到最终问题的最优解\r\n证明方法\r\n\r\n微扰(邻项交换)：证明再任意局面下，任何对局部最优策略的微小改变都会造成整体结果变差。(常用于以“排序”为贪心策略的证明)\r\n范围缩放：证明任何对局部最优策略作用范围的扩展都不会造成整体结果变差\r\n决策包容性：证明在任意局面下，做出局部最优决策后，在问题状态空间的可达集合包含了作出其他任何决策后的可达集合。换言之，这个局部最优决策提供的可能性包含其他所有策略提供的可能性。\r\n反证法：交换方案中任意两个元素，如果答案没变好，贪心即最优解\r\n归纳法：先找出边界情况的最优解F1，然后再证明：对每个n，Fn+1都能由Fn推出\r\n\r\n常见题型\r\n\r\n「我们将 XXX\r\n按照某某顺序排序，然后按某种顺序（例如从小到大）选择。」\r\n「我们每次都取 XXX 中最大/小的东西，并更新 XXX」（有时「XXX\r\n中最大/小的东西」可以优化，比如用优先队列维护）\r\n背包相关问题：给出n个物体，第i个物体重量为wi。选择尽量多的物体，使得总重量不超过C\r\n区间相关问题：数轴上有n个开区间(ai,bi)。选择尽量多区间，使这些区间两两没有公共点\r\nHuffman编码：给出n个字符的频率ci,给每个字符赋予一个01编码串，使得任意一个字符的编码不是另一个字符编码的前缀，而且编码后的总长度尽量小\r\n\r\n例题\r\n区间相关问题 1. 按右端点排序，取与之前不冲突的区间 2.\r\n求最大相交区间\r\n","slug":"贪心算法","date":"2022-01-13T02:24:15.000Z","categories_index":"算法","tags_index":"算法","author_index":"碔砆"},{"id":"f39f5057e79f1bc6b78f8eec0e6063d5","title":"最小公倍数LCM","content":"最小公倍数LCM\r\n原理\r\n最大公约数法\r\n最小公倍数 = 两整数的乘积 ➗ 最大公倍数\r\n前置知识——最大公因数GCD\r\n","slug":"最小公倍数LCM","date":"2022-01-12T10:29:18.000Z","categories_index":"算法","tags_index":"算法","author_index":"碔砆"}]