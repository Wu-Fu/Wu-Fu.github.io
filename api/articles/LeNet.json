{"title":"LeNet","uid":"139386a48ee9ff8eae54fecf132447a0","slug":"LeNet","date":"2023-02-20T10:33:27.000Z","updated":"2023-02-20T12:45:01.242Z","comments":true,"path":"api/articles/LeNet.json","keywords":null,"cover":[],"content":"<h1 id=\"lenet\">LeNet</h1>\r\n<h2 id=\"introduction\">Introduction</h2>\r\n<p>这是世界上最早发布的卷积神经网络之一，由AT&amp;T贝尔实验室的研究员Yann\r\nLeCun在1989年提出的（并以其命名）</p>\r\n<p>目的是识别图像 MNIST 中的手写数字</p>\r\n<p>当时，Yann\r\nLeCun发表了第一篇通过反向传播成功训练卷积神经网络的研究，这项工作代表了十多年来神经网络研究开发的成果</p>\r\n<h2 id=\"模型结构\">模型结构</h2>\r\n<p>整体上看，LeNet主要分为两个部分： - 卷积编码器：由两个卷积层组成 -\r\n全连接层密集块：由三个全连接层组成</p>\r\n<p><img src=\"https://zh.d2l.ai/_images/lenet.svg\">\r\n每个卷积块中的基本单元是一个卷积层、一个sigmoid激活函数和平均汇聚层</p>\r\n<p>每个卷积层使用<span class=\"math inline\"><mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.05ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"5.028ex\" height=\"1.557ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -666 2222.4 688\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mn\"><path data-c=\"35\" d=\"M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(722.2,0)\"><path data-c=\"D7\" d=\"M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z\"></path></g><g data-mml-node=\"mn\" transform=\"translate(1722.4,0)\"><path data-c=\"35\" d=\"M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z\"></path></g></g></g></svg></mjx-container></span>卷积核和一个sigmoid激活函数</p>\r\n<p>这些层将输入映射到多个二维特征输出，通常同时增加通道的数量</p>\r\n<p>为了将卷积块的输出传递给稠密块，我们必须在小批量中展平每个样本</p>\r\n<p>换言之，我们将这个四维输入转换成全连接层所期望的二维输入</p>\r\n<p>这里的二维表示的第一个维度索引小批量中的样本，第二个维度给出每个样本的平面向量表示</p>\r\n<p>LeNet的稠密块有三个全连接层，分别有120、84和10个输出</p>\r\n<h2 id=\"代码\">代码</h2>\r\n<p><a href=\"https://zhuanlan.zhihu.com/p/41736894\">LeNet详解</a></p>\r\n<p><a href=\"https://www.bilibili.com/video/BV1t44y1r7ct/?p=2&amp;vd_source=0010e78b7a63f02a4bea165f8571cf1c\">李沐——LeNet</a></p>\r\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">import torch\nfrom torch import nn\n\nclass Reshape(torch.nn.module):\n\t# 用于在通道数不变的情况下调整输入\n\tdef forward(self, x):\n\t\treturn x.view(-1, 1, 28, 28)\n\n\nnet = nn.Sequential(\n\tReshape(),\n\t# input层，输入为1*32*32的图片\n\tnn.Conv2d(1, 6, kernal_size=5, padding=2), nn.Sigmod(),\n\t# C1,6个5*5大小的卷积核，无填充，处理后数据变为28*28*6\n\tnn.AvgPool2d(kernal_size=2, stride=2),\n\t# S2，降采样层，先对2*2的视野进行平均，然后进入激活函数，处理后数据变为14*14*6\n\tnn.Conv2d(6, 16, kernal_size=5), nn.Sigmod(),\n\t# C3，16个大小为5x5的卷积核，步长为1。但是，这一层16个卷积核中只有10个和前面的6层相连接\n\t# 也就是说，这16个卷积核并不是扫描前一层所有的6个通道。而是只扫描其中的三个\n\t# 原因：破图像的对称性，期望学到互补的特征；减少连接的数量\n\tnn.AvgPool2d(kernal_size=2, stride=2),\n\tnn.Flatten(),\n\t# 把数据平铺成一个一维张量\n\tnn.Linear(16 * 5 * 5, 120), nn.Sigmod(),\n\t# 稠密全连接层，将数据逐层处理，直至最终的10类别(手写数字的个数)\n\tnn.Linear(120, 84), nn.Sigmod(),\n\tnn.Linear(84, 10)\n)</code></pre>\r\n<p>随机输入，查看各层的输出</p>\r\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)\nfor layer in net:\n    X = layer(X)\n    print(layer.__class__.__name__,'output shape: \\t',X.shape)</code></pre>\r\n<h2 id=\"小结\">小结</h2>\r\n<ul>\r\n<li>在卷积神经网络中，我们组合使用卷积层、非线性激活函数和汇聚层</li>\r\n<li>为了构造高性能的卷积神经网络，我们通常对卷积层进行排列，逐渐降低其表示的空间分辨率，同时增加通道数</li>\r\n<li>在传统的卷积神经网络中，卷积块编码得到的表征在输出之前需由一个或多个全连接层进行处理</li>\r\n</ul>\r\n","text":"LeNet Introduction 这是世界上最早发布的卷积神经网络之一，由AT&amp;T贝尔实验室的研究员Yann LeCun在1989年提出的（并以其命名） 目的是识别图像 MNIST 中的手写数字 当时，Yann LeCun发表了第一篇通过反向传播成功训练卷积神经网络的...","link":"","photos":[],"count_time":{"symbolsCount":"1.6k","symbolsTime":"1 mins."},"categories":[{"name":"AI学习","slug":"AI学习","count":8,"path":"api/categories/AI学习.json"}],"tags":[{"name":"AI学习","slug":"AI学习","count":8,"path":"api/tags/AI学习.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#lenet\"><span class=\"toc-text\">LeNet</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#introduction\"><span class=\"toc-text\">Introduction</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84\"><span class=\"toc-text\">模型结构</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E4%BB%A3%E7%A0%81\"><span class=\"toc-text\">代码</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%B0%8F%E7%BB%93\"><span class=\"toc-text\">小结</span></a></li></ol></li></ol>","author":{"name":"碔砆","slug":"blog-author","avatar":"https://s1.ax1x.com/2023/02/20/pSXmfmj.jpg","link":"/","description":"BUPT AI专业大二学生","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{"title":"AlexNet","uid":"c6ff40f5693e190d69588e9c4620b737","slug":"AlexNet","date":"2023-02-20T12:55:00.000Z","updated":"2023-02-24T09:17:21.833Z","comments":true,"path":"api/articles/AlexNet.json","keywords":null,"cover":[],"text":"AlexNet 论文原址 跟李沐读论文——AlexNet 三步读法 1. 导览——标题、摘要、结论 标题 ImageNet Classification with Deep Convolutional Neural Networks 从中可以感受到几个重点： 数据集：ImageN...","link":"","photos":[],"count_time":{"symbolsCount":"2.4k","symbolsTime":"2 mins."},"categories":[{"name":"AI学习","slug":"AI学习","count":8,"path":"api/categories/AI学习.json"},{"name":"论文精读","slug":"AI学习/论文精读","count":1,"path":"api/categories/AI学习/论文精读.json"},{"name":"经典模型","slug":"AI学习/论文精读/经典模型","count":1,"path":"api/categories/AI学习/论文精读/经典模型.json"}],"tags":[{"name":"AI学习","slug":"AI学习","count":8,"path":"api/tags/AI学习.json"},{"name":"论文精读","slug":"论文精读","count":2,"path":"api/tags/论文精读.json"},{"name":"经典模型","slug":"经典模型","count":3,"path":"api/tags/经典模型.json"}],"author":{"name":"碔砆","slug":"blog-author","avatar":"https://s1.ax1x.com/2023/02/20/pSXmfmj.jpg","link":"/","description":"BUPT AI专业大二学生","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}},"next_post":{"title":"信息论","uid":"a0a717998dbf88c81be122d0bce3bca3","slug":"信息论","date":"2023-02-18T10:32:18.000Z","updated":"2023-02-18T10:44:30.881Z","comments":true,"path":"api/articles/信息论.json","keywords":null,"cover":null,"text":"Information Theory MLAPP 2.8 Preface 信息论(Information Theory)：应用数学的一个分支，涉及用紧凑的方法来表示数据（如数据压缩和编码），以及具有鲁棒性的储存和传输数据。在机器学习中，信息论常常应用于连续型变量 信息论的基本想法...","link":"","photos":[],"count_time":{"symbolsCount":"1.4k","symbolsTime":"1 mins."},"categories":[{"name":"AI学习","slug":"AI学习","count":8,"path":"api/categories/AI学习.json"}],"tags":[{"name":"AI学习","slug":"AI学习","count":8,"path":"api/tags/AI学习.json"}],"author":{"name":"碔砆","slug":"blog-author","avatar":"https://s1.ax1x.com/2023/02/20/pSXmfmj.jpg","link":"/","description":"BUPT AI专业大二学生","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}}}