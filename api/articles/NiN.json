{"title":"NiN","uid":"c8663499333ced86e35b7c271cd82e06","slug":"NiN","date":"2023-02-24T11:41:11.000Z","updated":"2023-02-24T12:55:45.211Z","comments":true,"path":"api/articles/NiN.json","keywords":null,"cover":[],"content":"<h1 id=\"nin网络中的网络\">NiN——网络中的网络</h1>\r\n<h2 id=\"inroduction\">Inroduction</h2>\r\n<p>LeNet、AlexNet和VGG都有一个共同的设计模式：</p>\r\n<p>通过一系列的卷积层与汇聚层来提取空间结构特征，然后通过全连接层对特征的表征进行处理</p>\r\n<p>AlexNet和VGG对LeNet的提升主要在于如何扩大和加深这两个模块</p>\r\n<p>而这之中就发现了一个问题：卷积层后的第一个全连接层的参数过于庞大</p>\r\n<p>那么，我们是否可以在DNN的前期训练中就使用全连接层呢？</p>\r\n<p>NiN——网络中的网络给出了解决方法</p>\r\n<h2 id=\"模型结构nin块\">模型结构——NiN块</h2>\r\n<p>回想一下，卷积层的输入和输出由四维张量组成，张量的每个轴分别对应样本、通道、高度和宽度</p>\r\n<p>另外，全连接层的输入和输出通常是分别对应于样本和特征的二维张量</p>\r\n<p>从中，NiN给出了一种构想：在每个像素处，对高度和宽度分别应用一个全连接层</p>\r\n<p>从另一个角度看，即将空间维度中的每个像素视为单个样本，将通道维度视为不同特征</p>\r\n<p>NiN块以一个普通卷积层开始，后面是两个<span class=\"math inline\"><mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: 0;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"5.028ex\" height=\"1.507ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -666 2222.4 666\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mn\"><path data-c=\"31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(722.2,0)\"><path data-c=\"D7\" d=\"M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z\"></path></g><g data-mml-node=\"mn\" transform=\"translate(1722.4,0)\"><path data-c=\"31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"></path></g></g></g></svg></mjx-container></span>的卷积层</p>\r\n<p>这两个<span class=\"math inline\"><mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: 0;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"5.028ex\" height=\"1.507ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -666 2222.4 666\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mn\"><path data-c=\"31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(722.2,0)\"><path data-c=\"D7\" d=\"M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z\"></path></g><g data-mml-node=\"mn\" transform=\"translate(1722.4,0)\"><path data-c=\"31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"></path></g></g></g></svg></mjx-container></span>卷积层充当带有ReLU激活函数的逐像素全连接层</p>\r\n<p>关于<span class=\"math inline\"><mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: 0;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"5.028ex\" height=\"1.507ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -666 2222.4 666\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mn\"><path data-c=\"31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(722.2,0)\"><path data-c=\"D7\" d=\"M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z\"></path></g><g data-mml-node=\"mn\" transform=\"translate(1722.4,0)\"><path data-c=\"31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"></path></g></g></g></svg></mjx-container></span>卷积层的性质详见\r\n卷积神经网络一文``</p>\r\n<p><img src=\"https://zh.d2l.ai/_images/nin.svg\"></p>\r\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">import torch\nfrom torch import nn\n\ndef nin_block(in_channels, out_channels, kernel_size, stride, padding)\n\treturn nn.Sequential(\n\t\tnn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n\t\tnn.ReLU(),\n\t\tnn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(),\n\t\tnn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU()\n\t)</code></pre>\r\n<h2 id=\"模型结构nin网络\">模型结构——NiN网络</h2>\r\n<p>NiN和AlexNet之间的一个显著区别是NiN完全取消了全连接层</p>\r\n<p>相反，NiN使用一个NiN块，其输出通道数等于标签类别的数量，最后放一个全局平均池化层，生成一个对数几率</p>\r\n<p>这么做可以显著减少模型所需的参数数量，但在实践中发现这样有时会增加训练时间</p>\r\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">NiN = nn.Sequential(\n\tnin_block(1, 96, kernel_size=11, stride=4, padding=0),\n\tnn.MaxPool2d(3, stride=2),\n\tnin_block(96, 256, kernel_size=5, stride=4, padding=2),\n\tnn.MaxPool2d(3, stride=2),\n\tnin_block(256, 384, kernel_size=3, stride=1, padding=1),\n\tnn.MaxPool2d(3, stride=2),\n\tnn.Dropout(p=0.5),\n\tnin_block(384, 10, kernel_size=3, stride=1, padding=1),\n\t# 这里的10由数据集决定\n\tnn.AdaptiveAvgPool2d((1,1)),\n\tnn.Flatten()\n\t# 将四维的输出转换为二维的输出，其形状大小为(批量大小， 10)\n)</code></pre>\r\n<p>NiN的提出主要是为了减少全连接所带来的参数过大，从而引发的对模型显存的挑战，但相应的，NiN对模型训练的速度也存在着一定的影响，在当下模型显存不断发展的情况下，NiN技术逐渐被淘汰</p>\r\n","text":"NiN——网络中的网络 Inroduction LeNet、AlexNet和VGG都有一个共同的设计模式： 通过一系列的卷积层与汇聚层来提取空间结构特征，然后通过全连接层对特征的表征进行处理 AlexNet和VGG对LeNet的提升主要在于如何扩大和加深这两个模块 而这之中就发现...","link":"","photos":[],"count_time":{"symbolsCount":"1.5k","symbolsTime":"1 mins."},"categories":[{"name":"AI学习","slug":"AI学习","count":12,"path":"api/categories/AI学习.json"},{"name":"经典模型","slug":"AI学习/经典模型","count":5,"path":"api/categories/AI学习/经典模型.json"}],"tags":[{"name":"AI学习","slug":"AI学习","count":12,"path":"api/tags/AI学习.json"},{"name":"经典模型","slug":"经典模型","count":6,"path":"api/tags/经典模型.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#nin%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C\"><span class=\"toc-text\">NiN——网络中的网络</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#inroduction\"><span class=\"toc-text\">Inroduction</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84nin%E5%9D%97\"><span class=\"toc-text\">模型结构——NiN块</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84nin%E7%BD%91%E7%BB%9C\"><span class=\"toc-text\">模型结构——NiN网络</span></a></li></ol></li></ol>","author":{"name":"碔砆","slug":"blog-author","avatar":"https://s1.ax1x.com/2023/02/20/pSXmfmj.jpg","link":"/","description":"BUPT AI专业大二学生","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{"title":"GoogLeNet","uid":"f466839ba23e30f0626022c248cd73f5","slug":"GoogLeNet","date":"2023-02-24T12:58:30.000Z","updated":"2023-02-27T11:21:45.654Z","comments":true,"path":"api/articles/GoogLeNet.json","keywords":null,"cover":[],"text":"GoogLeNet Introduction GoogLeNet吸收了NiN中串联网络的思想，并在此基础上做了改进 这篇论文的一个重点是解决了什么样大小的卷积核最合适的问题 本文的一个观点是，有时使用不同大小的卷积核组合是有利的 模型结构——Inception块 在GoogLeN...","link":"","photos":[],"count_time":{"symbolsCount":"3.2k","symbolsTime":"3 mins."},"categories":[{"name":"AI学习","slug":"AI学习","count":12,"path":"api/categories/AI学习.json"},{"name":"经典模型","slug":"AI学习/经典模型","count":5,"path":"api/categories/AI学习/经典模型.json"}],"tags":[{"name":"AI学习","slug":"AI学习","count":12,"path":"api/tags/AI学习.json"},{"name":"经典模型","slug":"经典模型","count":6,"path":"api/tags/经典模型.json"}],"author":{"name":"碔砆","slug":"blog-author","avatar":"https://s1.ax1x.com/2023/02/20/pSXmfmj.jpg","link":"/","description":"BUPT AI专业大二学生","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}},"next_post":{"title":"VGG","uid":"e488f86556a876ba1fc98ee3101d0802","slug":"VGG","date":"2023-02-24T08:01:55.000Z","updated":"2023-02-24T11:16:14.159Z","comments":true,"path":"api/articles/VGG.json","keywords":null,"cover":[],"text":"VGG VGG论文 d2l Introdution AlexNet的成果，证明了CNN在图像分类领域的强大性能，但它没有提供一个通用的模板来指导后续的研究人员设计新的网络 与芯片设计中工程师从放置晶体管到逻辑元件再到逻辑块的过程类似，神经网络架构的设计也逐渐变得更加抽象 研究人员...","link":"","photos":[],"count_time":{"symbolsCount":"1.5k","symbolsTime":"1 mins."},"categories":[{"name":"AI学习","slug":"AI学习","count":12,"path":"api/categories/AI学习.json"},{"name":"经典模型","slug":"AI学习/经典模型","count":5,"path":"api/categories/AI学习/经典模型.json"}],"tags":[{"name":"AI学习","slug":"AI学习","count":12,"path":"api/tags/AI学习.json"},{"name":"经典模型","slug":"经典模型","count":6,"path":"api/tags/经典模型.json"}],"author":{"name":"碔砆","slug":"blog-author","avatar":"https://s1.ax1x.com/2023/02/20/pSXmfmj.jpg","link":"/","description":"BUPT AI专业大二学生","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}}}