{"title":"贝叶斯统计","uid":"60fb8c126ebe52ae9e85a9be67ca6bb2","slug":"贝叶斯统计","date":"2023-02-18T10:23:28.000Z","updated":"2023-02-18T10:37:44.194Z","comments":true,"path":"api/articles/贝叶斯统计.json","keywords":null,"cover":null,"content":"<p>MLAPP 149 DL 85 ## 5.1 Introduction\r\n区别于频率派统计方法和基于估计单一值<span\r\nclass=\"math inline\">\\(\\theta\\)</span>的方法：基于该估计做所有的预测\r\n另一种方法是在做预测时就考虑所有可能的<span\r\nclass=\"math inline\">\\(\\theta\\)</span>，后者属于<strong>贝叶斯统计(Bayesian\r\nstatistics)</strong>\r\n贝叶斯统计的核心在于：使用后验分布来总结我们所知道的关于一组未知变量的一切\r\n频率派的视角是真实参数<span\r\nclass=\"math inline\">\\(\\theta\\)</span>是未知的定值，而点估计是考虑数据集上函数的随机变量\r\n而贝叶斯统计用概率来反映知识状态的确定性程度，数据集是可观测的，而真实参数<span\r\nclass=\"math inline\">\\(\\theta\\)</span>是未知或不确定的，因此表示为随机变量\r\n在观察到数据前，我们将<span\r\nclass=\"math inline\">\\(\\theta\\)</span>的已知知识表示为<strong>先验概率分布(prior\r\nprobability distribution)</strong> 简称为<strong>先验</strong>\r\n当我们观测到m个样本后，对第m+1个样本的预测如下： <span\r\nclass=\"math display\">\\[p(x^{(m+1)}|D)=\\int\r\np(x^{(m+1)}|\\theta)p(\\theta|D)d\\theta\\]</span>\r\n可以看出，每个具有正概率密度的<span\r\nclass=\"math inline\">\\(\\theta\\)</span>都有助于下一个样本的判断，其中贡献有后验密度本身加权\r\n而在观测完数据集D后，若我们仍然不确定<span\r\nclass=\"math inline\">\\(\\theta\\)</span>的值，那么这个不确定性会直接包含在之后的预测中\r\n当训练数据小时，贝叶斯方法通常泛化的很好，但在训练数据大时，所产生的计算成本很大\r\n## 5.2 后验分布的总结 ### 5.2.1 MAP估计\r\n通过计算后验均值、中值(median)和模态(mode)我们可以很轻易的得到一个未知量的点估计\r\n通常情况下，后验均值或中值是实值量的最佳选择，后验边缘向量是离散量的最佳选择\r\n然而最受欢迎，也最为常用的后验模型，也就是 <strong>最大后验(Maximum A\r\nPosteriori，MAP)</strong> 估计\r\nMAP估计将问题简化为了一个优化问题，通过选择后验概率最大的点或是在<span\r\nclass=\"math inline\">\\(\\theta\\)</span>是连续型的更常见情况下，概率密度最大的点:\r\n<span class=\"math display\">\\[\\theta_{MAP}=arg\\max_\\theta\\\r\np(\\theta|x)=arg \\max_\\theta\\ log\\ p(x|\\theta)+log\\ p(\\theta)\\]</span>\r\n右边的<span class=\"math inline\">\\(log\\\r\np(x|\\theta)\\)</span>对应着标准的对数似然项，<span\r\nclass=\"math inline\">\\(log\\ p(\\theta)\\)</span>对应着先验分布\r\n尽管MAP估计非常好用，但其并不完善，接下来将介绍一些它的缺点 #### 5.2.1.1\r\n没有不确定度的度量\r\nMAP估计及其他任何点估计都具有一个缺点：缺少一个对不确定度的度量值，这在许多问题中是非常重要的\r\n#### 5.2.1.2 插入MAP估计容易导致过拟合\r\n由于在机器学习中，我们更关注预测的准确性，而不是如何解释模型的参数\r\n但如果不去建模参数中的不确定性，则容易导致模型的预测分布过度自信，从而导致过拟合\r\n#### 5.2.1.3 模态是一个非典型点\r\n选择模态作为后验分布通常是一个很糟糕的选择，因为模态对分布而言通常是非典型的，例子可见MLAPP150F5.1\r\n究其原因是，模态是一个衡量零值的点，而均值和中值考虑了空间的体积\r\n同时，在偏态分布(例子见MLAPP150F5.1)中，也常发生模式为0，但均值不为0的现象\r\n这种分布经常出现在推断方差参数时，尤其是在层次模型中，这使得MAP的表现非常糟糕\r\n当MAP为首的模式不好用时，我们常通过决策理论来总结后验分布 ### 5.2.2\r\n可信区间\r\n5.2.1.1中提出，MAP估计缺少一个对不确定度的度量，而对一个标量参数<span\r\nclass=\"math inline\">\\(\\theta\\)</span>的置信度的度量就是其后验分布的宽度\r\n这一宽度可以用<span class=\"math inline\">\\(100(1-\\alpha)\\%\\)</span>的\r\n<strong>可信区间(credible interval)</strong> 来衡量，可以用区间<span\r\nclass=\"math inline\">\\(C=(l,u)\\)</span>来表示 <span\r\nclass=\"math display\">\\[C_{\\alpha}(D)=(l,u):P(l\\le\\theta\\le\\alpha|D)=1-\\alpha\\]</span>\r\n这样的区间也许有很多个，因此我们在每一尾中选择质量为<span\r\nclass=\"math inline\">\\((1-\\alpha)/2\\)</span>的子区间，称其为<strong>中心区间(central\r\ninterval)</strong>\r\n如果后验分布有一个已知的函数表达式，我们可以通过其分布函数<span\r\nclass=\"math inline\">\\(F\\)</span>来计算可信区间 <span\r\nclass=\"math display\">\\[l=F^{-1}(\\alpha/2),u=F^{-1}(1-\\alpha/2)\\]</span>\r\n而当我们不知道后验分布的形式时，我们可以从后验分布中选取样本，之后再使用<strong>蒙特卡罗近似</strong>计算后验分位数\r\n排序<span\r\nclass=\"math inline\">\\(S\\)</span>个样本，并沿着排序后的列表来找到第<span\r\nclass=\"math inline\">\\(\\alpha/S\\)</span>的样本，当<span\r\nclass=\"math inline\">\\(S\\rightarrow\\infty\\)</span>时，这个样本就成为真正的分位数\r\nTips：可信区间(credible interval)和置信区间(confidence\r\ninterval)要区分开，前者属于贝叶斯统计，后者属于概率统计 ### 5.2.3\r\n在比例差异中的推理\r\n在现实生活中的很常见的例子：商品1有90个好评和10个差评，商品2有2个好评0个差评，该如何选择\r\n对这一问题，我们常通过贝叶斯分析，这一方法也可推广到其他关于比例的情况下\r\n详细过程见MLAPP 155(TO DO)\r\n另一种方法就是通过蒙特卡罗抽样来近似得出二者的后验分布 ## 5.3\r\n贝叶斯模型选择\r\n在一个机器学习问题中，超参数的选择不当常常会引发过拟合或者欠拟合的后果，由此便引出了<strong>模型选择</strong>这一问题\r\n模型选择的一种方法就是利用机器学习基础2.3节中介绍的交叉验证来估计所选模型的泛化误差\r\n而另一种更有效的方法就是计算所选模型的后验分布 <span\r\nclass=\"math display\">\\[p(m|D)=\\frac{p(D|m)p(m)}{\\sum_{m\\in\r\nM}p(m,D)}\\]</span> 从中，我们可以很快的计算出MAP模型：<span\r\nclass=\"math inline\">\\(\\hat{m}=argmax\\\r\np(m|D)\\)</span>，这就是<strong>贝叶斯模型选择</strong>\r\n如果我们使用一致的先验分布，<span class=\"math inline\">\\(p(m)\\propto\r\n1\\)</span>，则相当于选择最大化的模型 <span\r\nclass=\"math display\">\\[p(m|D)=\\int\r\np(D|\\theta)p(\\theta|m)d\\theta\\]</span>我们称这个量为\r\n<strong>边际似然(marginal likelihood)</strong>\r\n、<strong>微分似然(integrated likelihood)</strong> 或\r\n<strong>模型m的证据(evidence)</strong></p>\r\n","feature":true,"text":"MLAPP 149 DL 85 ## 5.1 Introduction 区别于频率派统计方法和基于估计单一值\\(\\theta\\)的方法：基于该估计做所有的预测 另一种方法是在做预测时就考虑所有可能的\\(\\theta\\)，后者属于贝叶斯统计(Bayesian statistics)...","link":"","photos":[],"count_time":{"symbolsCount":"2.6k","symbolsTime":"2 mins."},"categories":[],"tags":[],"toc":"","author":{"name":"碔砆","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"BUPT AI专业大二学生","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{"title":"卷积神经网络","uid":"665540b83d48c62e220c6e1d58a9965e","slug":"卷积神经网络","date":"2023-02-18T10:24:15.000Z","updated":"2023-02-18T10:24:15.325Z","comments":true,"path":"api/articles/卷积神经网络.json","keywords":null,"cover":null,"text":" ","link":"","photos":[],"count_time":{"symbolsCount":2,"symbolsTime":"1 mins."},"categories":[],"tags":[],"author":{"name":"碔砆","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"BUPT AI专业大二学生","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true},"next_post":{"title":"机器学习基础","uid":"e9525b7ef8e5ead70d9f4aba30aa5c7e","slug":"机器学习基础","date":"2023-02-18T08:33:33.000Z","updated":"2023-02-18T09:53:49.795Z","comments":true,"path":"api/articles/机器学习基础.json","keywords":null,"cover":null,"text":"机器学习基础 DL 1.学习算法 机器学习算法是一种能够从数据中学习的算法，可定义为 ”对于某类任务T和性能度量P，一个计算机程序被认为可以从经验E中学习是指，通过经验E改进后，它在任务T上由性能度量P衡量的性能有所提升\" 1.1 任务T 学习的过程本身并不是任务，学习是所谓获取...","link":"","photos":[],"count_time":{"symbolsCount":"2.9k","symbolsTime":"3 mins."},"categories":[{"name":"AI学习","slug":"AI学习","count":1,"path":"api/categories/AI学习.json"}],"tags":[{"name":"AI学习","slug":"AI学习","count":1,"path":"api/tags/AI学习.json"}],"author":{"name":"碔砆","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"BUPT AI专业大二学生","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}}}