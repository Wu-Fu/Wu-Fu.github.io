{"title":"机器学习基础","uid":"e9525b7ef8e5ead70d9f4aba30aa5c7e","slug":"机器学习基础","date":"2023-02-18T08:33:33.000Z","updated":"2023-02-18T08:35:00.261Z","comments":true,"path":"api/articles/机器学习基础.json","keywords":null,"cover":null,"content":"<h1 id=\"机器学习基础\"><a href=\"#机器学习基础\" class=\"headerlink\" title=\"机器学习基础\"></a>机器学习基础</h1><p>DL</p>\n<h2 id=\"1-学习算法\"><a href=\"#1-学习算法\" class=\"headerlink\" title=\"1.学习算法\"></a>1.学习算法</h2><p>机器学习算法是一种能够从数据中学习的算法，可定义为<br><strong>”对于某类任务T和性能度量P，一个计算机程序被认为可以从经验E中学习是指，通过经验E改进后，它在任务T上由性能度量P衡量的性能有所提升”</strong></p>\n<h3 id=\"1-1-任务T\"><a href=\"#1-1-任务T\" class=\"headerlink\" title=\"1.1 任务T\"></a>1.1 任务T</h3><p>学习的过程本身并不是任务，学习是所谓获取完成任务的能力的过程<br>通常来讲，机器学习任务被定义为 机器学习系统应该如何处理 <strong>样本(example)</strong><br>样本是指我们从某些希望机器学习系统处理的对象或事件中收集到的已经量化的 <strong>特征(feature)</strong> 的集合</p>\n<h3 id=\"1-2-性能度量P\"><a href=\"#1-2-性能度量P\" class=\"headerlink\" title=\"1.2 性能度量P\"></a>1.2 性能度量P</h3><p>为评估机器学习算法的能力，设计其性能的定量度量是有必要的<br>通常性能度量P是特定于任务T的，如分类型任务，性能度量P为准确率acc<br>对性能度量P的衡量主要聚焦于未观测数据，即 <strong>测试集(test set)</strong> 上<br>对于很多模型，选择一个与系统理想表现对应的性能度量通常是很有难度的</p>\n<h3 id=\"1-3-经验E\"><a href=\"#1-3-经验E\" class=\"headerlink\" title=\"1.3 经验E\"></a>1.3 经验E</h3><p>根据学习过程中的不同经验，机器学习算法可以大致分类为 <strong>无监督算法</strong> 和 <strong>有监督算法</strong><br>二者间主要区别来自于这样的一个视角：老师提供目标y给机器学习系统，指导其应该作什么。在无监督算法中则没有老师这样的角色，算法必须在没有指导的情况下理解数据</p>\n<h2 id=\"2-容量、过拟合和欠拟合\"><a href=\"#2-容量、过拟合和欠拟合\" class=\"headerlink\" title=\"2. 容量、过拟合和欠拟合\"></a>2. 容量、过拟合和欠拟合</h2><p>机器学习的重要目的就是在观测到的数据，即 <strong>训练集</strong> 上的表现良好之外，也在测试集上表现良好<br>这之中主要涉及一个算法的 <strong>泛化(generalization)</strong> 能力<br>统计学习理论对泛化能力的提升给出了相应的方法：<br>训练集和测试集数据通过数据集上被称为 <strong>数据生成过程(data generating process)</strong> 的概率分布生成。而在这一过程中采取 <strong>独立同分布(i.i.d assumption)</strong> 的假设，对提升泛化能力非常重要<br>独立同分布指，每个数据集中的样本都是彼此相互独立的，并且训练集和测试集是同分布的<br>这么做可以使得随机模型的训练误差期望和测试误差期望是一致的<br>从中，我们可以延申出决定机器学习算法效果是否好的两个因素：</p>\n<ul>\n<li>降低训练误差</li>\n<li>缩小训练误差和测试误差的差距</li>\n</ul>\n<p>而这两个因素又分别对应着机器学习的两个主要挑战：<strong>欠拟合(underfitting)</strong> 和 <strong>过拟合(overfitting)</strong><br>欠拟合指模型不能在训练集上获得足够低的误差，过拟合指训练误差和测试误差之间的差距过大<br>由此又另外延申出一个模型的概念：<strong>容量(capacity)</strong><br>容量指一个模型拟合各种函数的能力，容量过高则会使模型记住了不适于测试集的训练集性质从而表现为过拟合<br>一种常见的控制容量的方法是选择 <strong>假设空间(hypothesis space)</strong> ，即学习算法的选择范围为解决方法的函数集<br>主要控制容量的方法是：改变输入特征的数目和加入这些特征对应的参数<br>而在这一过程中，应当遵循 <strong>奥卡姆剃刀原理</strong><br>对于容量任意高的极端情况，则归于 <strong>非参数模型</strong> 的概念<br>非参数模型，指模型的参数会随着输入特征的变多而变多<br>与参数模型相比，非参数模型更为灵活，所需要的假设更少，但一个严重的问题在于对于大部分数据而言计算困难</p>\n<h3 id=\"2-1-没有免费午餐定理\"><a href=\"#2-1-没有免费午餐定理\" class=\"headerlink\" title=\"2.1 没有免费午餐定理\"></a>2.1 没有免费午餐定理</h3><p>对于不同的问题，通常需要设计不同的模型，而学习理论也表明机器学习算法可以在有限个训练集样本中很好地泛化<br>但是，必须强调，不存在万能的最佳模型，即 <strong>没有免费午餐定理(no free lunch theorem)</strong><br>这一理论表明，在所有可能的数据生成分布上平均之后，每一个分类算法在未事先观测的点上具有相同的错误率<br>但必须强调，这一定理仅在考虑所有可能的数据生成分布时才成立<br>在实际应用中，我们常对所遇到的概率分布进行假设，并针对该假设设计表现良好的算法</p>\n<h3 id=\"2-2-正则化-TO-DO\"><a href=\"#2-2-正则化-TO-DO\" class=\"headerlink\" title=\"2.2 正则化(TO DO)\"></a>2.2 正则化(TO DO)</h3><p> <strong>正则化(regularization)</strong> 是指修改学习算法，使其降低泛化误差而非训练误差</p>\n<h3 id=\"2-3-超参数和验证集\"><a href=\"#2-3-超参数和验证集\" class=\"headerlink\" title=\"2.3 超参数和验证集\"></a>2.3 超参数和验证集</h3><p> <strong>超参数(hyper-parameter)</strong> 常被设置用以控制算法的行为，超参数不是通过学习算法本身学习出来的<br>常见的设置超参数的原因是该参数不适合在训练集上学习，如控制模型容量的所有超参数<br>如果在训练集上训练这些超参数，这些超参数总是会区域最大可能的模型容量从而导致过拟合<br>为解决这个问题，引出一个训练算法观测不到的 <strong>验证集(validation set)</strong> 样本<br>验证集重要用于挑选模型的超参数<br>验证集的重点在于<strong>测试样本不能以任何形式参与到模型的选择之中</strong>，包括设置超参数<br>也是因此，之前提到的测试集中的样本不能用于验证集<br>通常情况下，经常将用于学习参数的训练集中挑选子集来构建验证集，比例常见为4：1</p>\n<h4 id=\"2-3-1-交叉验证\"><a href=\"#2-3-1-交叉验证\" class=\"headerlink\" title=\"2.3.1 交叉验证\"></a>2.3.1 交叉验证</h4><p>在训练中，若训练集的误差很小，在数据集太小时，可能会带来一些问题<br>一个小规模的测试集意味着平均测试误差估计的统计不确定性，使得很难判断算法A、B之间在给定任务上的优劣<br>常见的一个解决方法为<strong>k-fold交叉验证</strong><br>算法如下：将训练集分为k个互斥子集，训练除第k个子集外的每一个子集，并在第k个子集上进行测试，以这种方式进行滚动式循环训练。之后，计算所有子集的平均误差，并以此来代替测试误差<br>（tips：每个点只用于一次测试，和n-1次训练）</p>\n<h3 id=\"2-4-估计、偏差和方差（TO-DO）\"><a href=\"#2-4-估计、偏差和方差（TO-DO）\" class=\"headerlink\" title=\"2.4 估计、偏差和方差（TO DO）\"></a>2.4 估计、偏差和方差（TO DO）</h3><h4 id=\"2-4-1-点估计\"><a href=\"#2-4-1-点估计\" class=\"headerlink\" title=\"2.4.1 点估计\"></a>2.4.1 点估计</h4><h4 id=\"2-4-2-偏差\"><a href=\"#2-4-2-偏差\" class=\"headerlink\" title=\"2.4.2 偏差\"></a>2.4.2 偏差</h4><h4 id=\"2-4-3-方差和标准差\"><a href=\"#2-4-3-方差和标准差\" class=\"headerlink\" title=\"2.4.3 方差和标准差\"></a>2.4.3 方差和标准差</h4><h4 id=\"2-4-4-权衡偏差和方差以最小化均分误差\"><a href=\"#2-4-4-权衡偏差和方差以最小化均分误差\" class=\"headerlink\" title=\"2.4.4 权衡偏差和方差以最小化均分误差\"></a>2.4.4 权衡偏差和方差以最小化均分误差</h4><h4 id=\"2-4-5-一致性\"><a href=\"#2-4-5-一致性\" class=\"headerlink\" title=\"2.4.5 一致性\"></a>2.4.5 一致性</h4><h3 id=\"2-5-最大似然估计\"><a href=\"#2-5-最大似然估计\" class=\"headerlink\" title=\"2.5 最大似然估计\"></a>2.5 最大似然估计</h3><p>MLAPP 217<br>估计统计模型参数$\\theta$的一种常见方法是计算其<strong>最大似然估计(Maximum likelihood estimation，MLE)</strong><br>对$\\theta$的<strong>最大似然估计</strong>定义为：<br>$$<br>\\begin{aligned}<br>\\theta_{ML}&amp;=\\arg \\max_\\theta p_{model}(X;\\theta)\\<br>&amp;=\\arg \\max_\\theta \\prod_{i=1}^{m} p_{model}(x^{(i)};\\theta)<br>\\end{aligned}$$<br>为简便运算，常将原函数进行取对数处理<br>在实际进行代码运行时，我们更经常等价地采取最小化负对数的似然<br>一种解释最大似然估计的观点是将它看作最小化训练集上的经验分布和数据之间的差异，二者间的差异程度由KL散度度量<br>$$KL(p||q)=E_{x\\sim \\hat p_{data}}[log\\ \\hat p_{data}(x)-log\\ p_{data}(x)]$$<br>其中左边一项仅涉及数据生成过程，因此只需要最小化$-E_{x\\sim \\hat p_{data}}[log\\ p_{data}(x)]$</p>\n<h4 id=\"2-5-1-条件对数似然和均方误差（to-do）\"><a href=\"#2-5-1-条件对数似然和均方误差（to-do）\" class=\"headerlink\" title=\"2.5.1 条件对数似然和均方误差（to do）\"></a>2.5.1 条件对数似然和均方误差（to do）</h4><p>由之前最大似然估计的定义，可进一步拓展到估计条件概率$P(y|x;\\theta)$从而给定x预测y<br>而这就构成了大多数监督学习的基础</p>\n<h4 id=\"2-5-2-最大似然的性质\"><a href=\"#2-5-2-最大似然的性质\" class=\"headerlink\" title=\"2.5.2 最大似然的性质\"></a>2.5.2 最大似然的性质</h4><p>最大似然估计的最重要的一点在于，它被证明了当样本数目$m\\rightarrow \\infty$时，就收敛率而言是最好的渐进估计<br>在以下条件下，最大似然估计具有一致性</p>\n<ul>\n<li>真实分布$p_{data}$必须在模型族$p_{model}(\\cdot ;\\theta)$中，否则，没有估计科研还原$p_{data}$</li>\n<li>真实分布$p_{data}$必须刚好对应一个$\\theta$值，否则，最大似然估计恢复除真实分布$p_{data}$后，也不能决定数据生成过程使用哪个$\\theta$<br>除了最大似然估计之外，还有其他的归纳准则，且都具有一致性<br>但这些一致估计的 <strong>统计效率(statistic efficiency)</strong> 可能区别很大<br>在综合考虑了一致性和统计效率之后，最大似然通常是机器学习中的首选估计方法<h3 id=\"2-6-贝叶斯统计\"><a href=\"#2-6-贝叶斯统计\" class=\"headerlink\" title=\"2.6 贝叶斯统计\"></a>2.6 贝叶斯统计</h3><h3 id=\"2-7-监督学习算法\"><a href=\"#2-7-监督学习算法\" class=\"headerlink\" title=\"2.7 监督学习算法\"></a>2.7 监督学习算法</h3><h3 id=\"2-8-无监督学习算法\"><a href=\"#2-8-无监督学习算法\" class=\"headerlink\" title=\"2.8 无监督学习算法\"></a>2.8 无监督学习算法</h3><h3 id=\"2-9-随机梯度下降\"><a href=\"#2-9-随机梯度下降\" class=\"headerlink\" title=\"2.9 随机梯度下降\"></a>2.9 随机梯度下降</h3></li>\n</ul>\n","feature":true,"text":"机器学习基础DL 1.学习算法机器学习算法是一种能够从数据中学习的算法，可定义为”对于某类任务T和性能度量P，一个计算机程序被认为可以从经验E中学习是指，通过经验E改进后，它在任务T上由性能度量P衡量的性能有所提升” 1.1 任务T学习的过程本身并不是任务，学习是所谓获取完成任务...","link":"","photos":[],"count_time":{"symbolsCount":"3.1k","symbolsTime":"3 mins."},"categories":[],"tags":[{"name":"AI","slug":"AI","count":1,"path":"api/tags/AI.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80\"><span class=\"toc-text\">机器学习基础</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#1-%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95\"><span class=\"toc-text\">1.学习算法</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#1-1-%E4%BB%BB%E5%8A%A1T\"><span class=\"toc-text\">1.1 任务T</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#1-2-%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8FP\"><span class=\"toc-text\">1.2 性能度量P</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#1-3-%E7%BB%8F%E9%AA%8CE\"><span class=\"toc-text\">1.3 经验E</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#2-%E5%AE%B9%E9%87%8F%E3%80%81%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88\"><span class=\"toc-text\">2. 容量、过拟合和欠拟合</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-1-%E6%B2%A1%E6%9C%89%E5%85%8D%E8%B4%B9%E5%8D%88%E9%A4%90%E5%AE%9A%E7%90%86\"><span class=\"toc-text\">2.1 没有免费午餐定理</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-2-%E6%AD%A3%E5%88%99%E5%8C%96-TO-DO\"><span class=\"toc-text\">2.2 正则化(TO DO)</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-3-%E8%B6%85%E5%8F%82%E6%95%B0%E5%92%8C%E9%AA%8C%E8%AF%81%E9%9B%86\"><span class=\"toc-text\">2.3 超参数和验证集</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#2-3-1-%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81\"><span class=\"toc-text\">2.3.1 交叉验证</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-4-%E4%BC%B0%E8%AE%A1%E3%80%81%E5%81%8F%E5%B7%AE%E5%92%8C%E6%96%B9%E5%B7%AE%EF%BC%88TO-DO%EF%BC%89\"><span class=\"toc-text\">2.4 估计、偏差和方差（TO DO）</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#2-4-1-%E7%82%B9%E4%BC%B0%E8%AE%A1\"><span class=\"toc-text\">2.4.1 点估计</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#2-4-2-%E5%81%8F%E5%B7%AE\"><span class=\"toc-text\">2.4.2 偏差</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#2-4-3-%E6%96%B9%E5%B7%AE%E5%92%8C%E6%A0%87%E5%87%86%E5%B7%AE\"><span class=\"toc-text\">2.4.3 方差和标准差</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#2-4-4-%E6%9D%83%E8%A1%A1%E5%81%8F%E5%B7%AE%E5%92%8C%E6%96%B9%E5%B7%AE%E4%BB%A5%E6%9C%80%E5%B0%8F%E5%8C%96%E5%9D%87%E5%88%86%E8%AF%AF%E5%B7%AE\"><span class=\"toc-text\">2.4.4 权衡偏差和方差以最小化均分误差</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#2-4-5-%E4%B8%80%E8%87%B4%E6%80%A7\"><span class=\"toc-text\">2.4.5 一致性</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-5-%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1\"><span class=\"toc-text\">2.5 最大似然估计</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#2-5-1-%E6%9D%A1%E4%BB%B6%E5%AF%B9%E6%95%B0%E4%BC%BC%E7%84%B6%E5%92%8C%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE%EF%BC%88to-do%EF%BC%89\"><span class=\"toc-text\">2.5.1 条件对数似然和均方误差（to do）</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#2-5-2-%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E7%9A%84%E6%80%A7%E8%B4%A8\"><span class=\"toc-text\">2.5.2 最大似然的性质</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-6-%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BB%9F%E8%AE%A1\"><span class=\"toc-text\">2.6 贝叶斯统计</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-7-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95\"><span class=\"toc-text\">2.7 监督学习算法</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-8-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95\"><span class=\"toc-text\">2.8 无监督学习算法</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-9-%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D\"><span class=\"toc-text\">2.9 随机梯度下降</span></a></li></ol></li></ol></li></ol>","author":{"name":"碔砆","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{},"next_post":{"title":"STL","uid":"48433a70b7e09e957d48f01fa1012852","slug":"STL","date":"2022-01-15T12:53:20.000Z","updated":"2023-02-18T07:58:18.522Z","comments":true,"path":"api/articles/STL.json","keywords":null,"cover":null,"text":"STLSTL——OIWiki 共同点 声明形式： 容器名&lt;数据类型&gt; 变量名 迭代器：用来访问和检查STL容器中的元素的对象，与数据指针类似。主要支持自增（++）和解引用（*）运算符，其中自增用来移动迭代器，解引用可以获取或修改它指向的元素。 用法：容器名&lt;数据...","link":"","photos":[],"count_time":{"symbolsCount":"5.2k","symbolsTime":"5 mins."},"categories":[{"name":"算法","slug":"算法","count":2,"path":"api/categories/算法.json"}],"tags":[{"name":"算法","slug":"算法","count":2,"path":"api/tags/算法.json"}],"author":{"name":"碔砆","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true}}